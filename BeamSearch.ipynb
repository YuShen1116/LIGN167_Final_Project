{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils import clip_grad_norm_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "\n",
    "# import basic lib\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import string\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "from io import open\n",
    "\n",
    "# import pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "# import loss func\n",
    "import masked_cross_entropy\n",
    "import copy\n",
    "# check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_idx = 0\n",
    "EOS_idx = 1\n",
    "UNK_idx = 2\n",
    "PAD_idx = 3\n",
    "\n",
    "USE_CUDA = True\n",
    "\n",
    "class Preprocessor:\n",
    "    '''\n",
    "    class for preprocessing\n",
    "    '''\n",
    "    def __init__(self, name):\n",
    "        '''\n",
    "        initialize vocab and counter\n",
    "        '''\n",
    "        self.name = name\n",
    "        self.w2idx = {\"<sos>\" : 0, \"<eos>\" : 1, \"<unk>\" : 2, \"<pad>\" : 3}\n",
    "        self.counter = {}\n",
    "        self.idx2w = {0: \"<sos>\", 1: \"<eos>\", 2:\"<unk>\", 3:\"<pad>\"}\n",
    "        self.num = 4\n",
    "\n",
    "    def SentenceAdder(self, sentence):\n",
    "        '''\n",
    "        Add a sentence to dataset\n",
    "        '''\n",
    "        for word in sentence.split(' '):\n",
    "            self.WordAdder(word)\n",
    "\n",
    "    def WordAdder(self, word):\n",
    "        '''\n",
    "        Add single word to dataset and update vocab and counter\n",
    "        '''\n",
    "        if word in self.w2idx:\n",
    "            self.counter[word] += 1\n",
    "        else:\n",
    "            self.w2idx[word] = self.num\n",
    "            self.counter[word] = 1\n",
    "            self.idx2w[self.num] = word\n",
    "            self.num += 1\n",
    "            \n",
    "    def trim(self, min_count=1):\n",
    "        '''\n",
    "        Trim to remove non-frequent word\n",
    "        '''\n",
    "        keep = []\n",
    "        for k, v in self.counter.items():\n",
    "            if v >= min_count: keep.append(k)\n",
    "        print(self.name+':')\n",
    "        print('Total words', len(self.w2idx))\n",
    "        print('After Trimming', len(keep))\n",
    "        print('Keep Ratio %', 100 * len(keep) / len(self.w2idx))\n",
    "        self.w2idx = {\"<sos>\" : 0, \"<eos>\" : 1, \"<unk>\" : 2, \"<pad>\" : 3}\n",
    "        self.counter = {}\n",
    "        self.idx2w = {0: \"<sos>\", 1: \"<eos>\", 2:\"<unk>\", 3:\"<pad>\"}\n",
    "        self.num = 4\n",
    "        for w in keep:\n",
    "            self.WordAdder(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Uni2Ascii(s):\n",
    "    '''\n",
    "    transfer from unicode to ascii\n",
    "    '''\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                    if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def StrCleaner(s):\n",
    "    '''\n",
    "    trim, delete non-letter and lowercase string\n",
    "    '''\n",
    "    s = Uni2Ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def DataReader(path, lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open(path, encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    #pairs = [[StrCleaner(s) for s in l.split('<------>')] for l in lines]\n",
    "    pairs = [[s.lower() for s in l.split('<------>')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Preprocessor(lang2)\n",
    "        output_lang = Preprocessor(lang1)\n",
    "    else:\n",
    "        input_lang = Preprocessor(lang1)\n",
    "        output_lang = Preprocessor(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_LENGTH = 3\n",
    "MAX_LENGTH = 50\n",
    "\n",
    "def filterPair(p):\n",
    "    '''\n",
    "    Filter to get expected pairs with specific length\n",
    "    '''\n",
    "    return MIN_LENGTH <= len(p[0].split(' ')) <= MAX_LENGTH and \\\n",
    "        MIN_LENGTH <= len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 90000 sentence pairs\n",
      "Trimmed to 77083 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "chinese 46064\n",
      "english 30499\n",
      "chinese:\n",
      "Total words 46064\n",
      "After Trimming 46060\n",
      "Keep Ratio % 99.99131642931573\n",
      "english:\n",
      "Total words 30499\n",
      "After Trimming 30495\n",
      "Keep Ratio % 99.9868848158956\n",
      "['\" 我 相信 , 这次 会议 必将 对 新世纪 的 中非 关系 产生 深远 影响 . \"', 'jiang zemin said , \" i believe the current meeting will surely exert profound impact on china - africa relations in the new century . \"']\n"
     ]
    }
   ],
   "source": [
    "def prepareData(path, lang1, lang2, reverse=True):\n",
    "    input_lang, output_lang, pairs = DataReader(path, lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.SentenceAdder(pair[0])\n",
    "        output_lang.SentenceAdder(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.num)\n",
    "    print(output_lang.name, output_lang.num)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "src, tgt, pairs = prepareData('data/train.txt', 'english', 'chinese')\n",
    "src.trim()\n",
    "tgt.trim()\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2idx(preprocessor, sentence):\n",
    "    '''\n",
    "    Read sentence and translate into word index plus eos\n",
    "    '''\n",
    "    return [SOS_idx] + [preprocessor.w2idx[w] if w in preprocessor.w2idx \\\n",
    "            else UNK_idx for w in sentence.split(' ')] + [EOS_idx]\n",
    "\n",
    "def pad(seq, max_len):\n",
    "    '''\n",
    "    Add padding to sentence with different length\n",
    "    '''\n",
    "    seq += [PAD_idx for i in range(max_len - len(seq))]\n",
    "    return seq\n",
    "\n",
    "def random_batch(src, tgt, pairs, batch_size, batch_idx):\n",
    "    '''\n",
    "    Randomly generate batch data\n",
    "    '''\n",
    "    inputs, target = [], []\n",
    "    \n",
    "    # Choose batch\n",
    "    for s in pairs[batch_idx*batch_size:(batch_idx+1)*batch_size]:\n",
    "        inputs.append(sentence2idx(src, s[0]))\n",
    "        target.append(sentence2idx(tgt, s[1]))\n",
    "        \n",
    "    # Sort by length\n",
    "    seq_pairs = sorted(zip(inputs, target), key=lambda p: len(p[0]), reverse=True)\n",
    "    inputs, target = zip(*seq_pairs)\n",
    "    \n",
    "    # Obtain length of each sentence and pad\n",
    "    input_lens = [len(s) for s in inputs]\n",
    "    input_max = max(input_lens)\n",
    "    input_padded = [pad(s, input_max) for s in inputs]\n",
    "    target_lens = [len(s) for s in target]\n",
    "    target_max = max(target_lens)\n",
    "    target_padded = [pad(s, target_max) for s in target]\n",
    "\n",
    "    # Create Variable\n",
    "    if USE_CUDA:\n",
    "        input_vars = Variable(torch.LongTensor(input_padded).cuda()).transpose(0, 1)\n",
    "        input_lens = Variable(torch.LongTensor(input_lens).cuda())\n",
    "        target_vars = Variable(torch.LongTensor(target_padded).cuda()).transpose(0, 1)\n",
    "        target_lens = Variable(torch.LongTensor(target_lens).cuda())\n",
    "    else:\n",
    "        input_vars = Variable(torch.LongTensor(input_padded)).transpose(0, 1)\n",
    "        input_lens = Variable(torch.LongTensor(input_lens))\n",
    "        target_vars = Variable(torch.LongTensor(target_padded)).transpose(0, 1)\n",
    "        target_lens = Variable(torch.LongTensor(target_lens))\n",
    "\n",
    "    return input_vars, input_lens, target_vars, target_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    '''\n",
    "    Define encoder and forward process\n",
    "    '''\n",
    "    def __init__(self, dim_input, dim_embed, dim_hidden, num_layers, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dim_input = dim_input\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.dim_embed = dim_embed\n",
    "        self.embed = nn.Embedding(dim_input, dim_embed)\n",
    "        self.cell = nn.GRU(dim_embed, dim_hidden, \n",
    "                          num_layers, dropout=dropout, \n",
    "                          bidirectional=True)\n",
    "        \n",
    "    def forward(self, inputs, inputs_lens, hidden=None):\n",
    "        '''\n",
    "        We need to sum the outputs since bi-diretional is used\n",
    "        '''\n",
    "        #print('e')\n",
    "        embedded = self.embed(inputs)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, inputs_lens)\n",
    "        outputs, hidden = self.cell(packed, hidden)\n",
    "        outputs, output_lengths = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        outputs = outputs[:, :, :self.dim_hidden] + \\\n",
    "                    outputs[:, :, self.dim_hidden:]\n",
    "        return outputs, hidden\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    '''\n",
    "    Define attention mechanism\n",
    "    '''\n",
    "    def __init__(self, dim_hidden):\n",
    "        super(Attention, self).__init__()\n",
    "        self.dim_hidden = dim_hidden\n",
    "        # 2*dim_hidden is needed since bi-direction is used\n",
    "        self.attn = nn.Linear(2*self.dim_hidden, dim_hidden)\n",
    "        self.v = nn.Parameter(torch.rand(dim_hidden))\n",
    "        stdv = 1. / math.sqrt(self.v.size(0))\n",
    "        self.v.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        #print('a')\n",
    "        timestep = encoder_outputs.size(0)\n",
    "        h = hidden.repeat(timestep, 1, 1).transpose(0, 1)\n",
    "        encoder_outputs = encoder_outputs.transpose(0, 1)\n",
    "        scores = self.score(h, encoder_outputs)\n",
    "        return F.relu(scores).unsqueeze(1)\n",
    "\n",
    "    def score(self, hidden, encoder_outputs):\n",
    "        e = F.softmax(self.attn(torch.cat([hidden, encoder_outputs], 2)),dim=1)\n",
    "        e = e.transpose(1, 2)\n",
    "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)\n",
    "        e = torch.bmm(v, e)\n",
    "        return e.squeeze(1)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    '''\n",
    "    Define decoder with attention\n",
    "    '''\n",
    "    def __init__(self, dim_embed, dim_hidden, dim_output, num_layers, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dim_embed = dim_embed\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.dim_output = dim_output\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embed = nn.Embedding(dim_output, dim_embed)\n",
    "        self.dropout = nn.Dropout(dropout, inplace=True)\n",
    "        self.attention = Attention(dim_hidden)\n",
    "        self.cell = nn.GRU(dim_hidden + dim_embed, dim_hidden,\n",
    "                          num_layers, dropout=dropout)\n",
    "        self.out = nn.Linear(2*dim_hidden, dim_output)\n",
    "\n",
    "    def forward(self, inputs, last_hidden, encoder_outputs):\n",
    "        \n",
    "#         print('input size ' + str(inputs.size()))\n",
    "#         print('input type ' + str(inputs.type()))\n",
    "        \n",
    "        embedded = self.embed(inputs).unsqueeze(0)  # (1,B,N)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "#         print('emmb size ' + str(embedded.size()))\n",
    "#         print('emmb type ' + str(embedded.type()))\n",
    "        \n",
    "#         input size torch.Size([1])\n",
    "# input type torch.cuda.LongTensor\n",
    "# emmb size torch.Size([1, 1, 256])\n",
    "# emmb type torch.cuda.FloatTensor\n",
    "        # Calculate attention weights and apply to encoder outputs\n",
    "        attn_weights = self.attention(last_hidden[-1], encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))  # (B,1,N)\n",
    "        context = context.transpose(0, 1)  # (1,B,N)\n",
    "        # Combine embedded input word and attended context, run through RNN\n",
    "\n",
    "        rnn_input = torch.cat([embedded, context], 2)\n",
    "        output, hidden = self.cell(rnn_input, last_hidden)\n",
    "        output = output.squeeze(0)  # (1,B,N) -> (B,N)\n",
    "        context = context.squeeze(0)\n",
    "        output = self.out(torch.cat([output, context], 1))\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, src_len, tgt, tgt_len, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.size(1)\n",
    "        max_len = tgt.size(0)\n",
    "        vocab_size = self.decoder.dim_output\n",
    "        if USE_CUDA:\n",
    "            outputs = Variable(torch.zeros(max_len, batch_size, vocab_size).cuda())\n",
    "        else:\n",
    "            outputs = Variable(torch.zeros(max_len, batch_size, vocab_size))\n",
    "        encoder_output, hidden = self.encoder(src, src_len)\n",
    "        hidden = hidden[:self.decoder.num_layers]\n",
    "        # Put <sos> at first position\n",
    "        if USE_CUDA:\n",
    "            output = Variable(tgt.data[0, :].cuda())\n",
    "        else:\n",
    "            output = Variable(tgt.data[0, :])\n",
    "        for t in range(1, max_len):\n",
    "            output, hidden, attn_weights = self.decoder(\n",
    "                    output, hidden, encoder_output)\n",
    "            outputs[t] = output\n",
    "            # Randomly choose whether to use teacher force or not\n",
    "            is_teacher = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.data.max(1)[1]\n",
    "            if USE_CUDA:\n",
    "                output = Variable(tgt.data[t].cuda() if is_teacher else top1.cuda())\n",
    "            else:\n",
    "                output = Variable(tgt.data[t] if is_teacher else top1)\n",
    "        return outputs\n",
    "    \n",
    "    def inference(self, src, src_len, max_len = MAX_LENGTH):\n",
    "        pred_idx = []\n",
    "        batch_size = src.size(1)\n",
    "        vocab_size = self.decoder.dim_output\n",
    "        if USE_CUDA:\n",
    "            outputs = Variable(torch.zeros(max_len, batch_size, vocab_size).cuda())\n",
    "        else:\n",
    "            outputs = Variable(torch.zeros(max_len, batch_size, vocab_size))\n",
    "        \n",
    "        encoder_output, hidden = self.encoder(src, src_len)\n",
    "        hidden = hidden[:self.decoder.num_layers]\n",
    "        # Put <sos> at first position\n",
    "        if USE_CUDA:\n",
    "            output = Variable(src.data[0, :].cuda())\n",
    "        else:\n",
    "            output = Variable(src.data[0, :])\n",
    "        pred_idx.append(SOS_idx)\n",
    "        for t in range(1, max_len):\n",
    "            output, hidden, attn_weights = self.decoder(\n",
    "                    output, hidden, encoder_output)\n",
    "            outputs[t] = output\n",
    "            top1 = output.data.max(1)[1]\n",
    "            pred_idx.append(top1.item())\n",
    "            if USE_CUDA:\n",
    "                output = Variable(top1.cuda())\n",
    "            else:\n",
    "                output = Variable(top1)\n",
    "            if top1 == EOS_idx: break\n",
    "        return outputs, pred_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\cywkh\\anaconda2\\envs\\lign167\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "hidden_size = 512\n",
    "embed_size = 256\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 1 \n",
    "# n_layers = 4\n",
    "width = 5\n",
    "encoder_test = Encoder(src.num, embed_size, hidden_size, encoder_n_layers, dropout=0.2)\n",
    "decoder_test = Decoder(embed_size, hidden_size, tgt.num, decoder_n_layers, dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Seq2Seq(encoder_test,decoder_test).cuda()\n",
    "# opt = optim.Adam(net.parameters(),lr=0.0001)\n",
    "net.load_state_dict(torch.load('./saved_model_1.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 10000 sentence pairs\n",
      "Trimmed to 8572 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "chinese 17294\n",
      "english 13049\n",
      "chinese:\n",
      "Total words 17294\n",
      "After Trimming 17290\n",
      "Keep Ratio % 99.9768705909564\n",
      "english:\n",
      "Total words 13049\n",
      "After Trimming 13045\n",
      "Keep Ratio % 99.96934631006208\n",
      "['二 者 缺一不可 , 也 不可偏废 . \"', 'neither is dispensable nor is overemphasized at the expense of the other . \"']\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "\n",
    "test_src, test_tgt, test_pairs = prepareData('data/test.txt', 'english', 'chinese')\n",
    "test_src.trim()\n",
    "test_tgt.trim()\n",
    "print(random.choice(test_pairs))\n",
    "\n",
    "test_src.w2idx, test_src.idx2w, test_src.num = src.w2idx, src.idx2w, src.num\n",
    "test_tgt.w2idx, test_tgt.idx2w, test_tgt.num = tgt.w2idx, tgt.idx2w, tgt.num\n",
    "test_pairs.sort(key=lambda x: len(x[0].split()))\n",
    "\n",
    "input_batches, input_lengths,\\\n",
    "    target_batches, target_lengths = random_batch(test_src,test_tgt,test_pairs,batch_size,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batches, input_lengths,\\\n",
    "    target_batches, target_lengths = random_batch(test_src,test_tgt,test_pairs,1,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0, 536, 503, 116, 297,   1], device='cuda:0')\n",
      "tensor([  0, 562,  26,  71, 128, 105,  79, 292, 331,   1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# print(input_batches.size())\n",
    "# print(target_batches.size())\n",
    "\n",
    "print(input_batches[:,0])\n",
    "print(target_batches[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearch(nn.Module):\n",
    "    '''\n",
    "    Implement BeamSearch for testing\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, encoder, decoder, width):\n",
    "        super(BeamSearch, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.width = width\n",
    "        \n",
    "    def forward(self, src, src_len, width, max_len):\n",
    "        vocab_size = self.decoder.dim_output        \n",
    "        encoder_output, hidden = self.encoder(src, src_len)\n",
    "#         _, initial_hidden = self.encoder(src, src_len)\n",
    "        hidden = hidden[:self.decoder.num_layers]\n",
    "#         initial_hidden = initial_hidden[:self.decoder.num_layers]\n",
    "        \n",
    "        if USE_CUDA:\n",
    "            #output is in the shape of batch_size * dict size\n",
    "            beam_best = Variable(torch.zeros(width, max_len)).cuda()\n",
    "            prob_best = Variable(torch.zeros(width)).cuda()\n",
    "            #beam options, all possible width * width options\n",
    "            beam_options = Variable(torch.zeros(width * width, max_len)).cuda()\n",
    "            prob_options = Variable(torch.zeros(width * width)).cuda()\n",
    "#             outputs = Variable(torch.zeros(width * width).cuda())\n",
    "            output = Variable(src.data[0, :].cuda())\n",
    "            temp = Variable(src.data[0, :].cuda())\n",
    "#             new_output = Variable(src.data[0, :].cuda())\n",
    "            #index with greatest probablity of size width\n",
    "#             best_output = Variable(torch.zeros(width).cuda())\n",
    "        else:\n",
    "            #output is in the shape of batch_size * dict size\n",
    "            beam_best = Variable(torch.zeros(width, max_len))\n",
    "            prob_best = Variable(torch.zeros(width))\n",
    "            #beam options, all possible width * width options\n",
    "            beam_options = Variable(torch.zeros(width * width, max_len))\n",
    "            prob_options = Variable(torch.zeros(width * width))\n",
    "            temp = Variable(src.data[0, :])\n",
    "#             outputs = Variable(torch.zeros(width * width))\n",
    "            output = Variable(src.data[0, :])\n",
    "#             new_output = Variable(src.data[0, :])\n",
    "            #index with greatest probablity of size width\n",
    "#             best_output = Variable(torch.zeros(width))\n",
    "        \n",
    "        output, _, _ = self.decoder(\n",
    "            output, hidden, encoder_output)\n",
    "        \n",
    "        #first round\n",
    "        #val: prob, idx: index\n",
    "        val, idx = output.data.topk(k = width, dim = 1)\n",
    "#         print(idx[0,:]) #tensor([  4, 261, 285,  40, 466], device='cuda:0')\n",
    "#         print(val[0,:]) #tensor([-0.0034, -6.9214, -7.3048, -7.4925, -8.7961], device='cuda:0')\n",
    "    \n",
    "        #generate beams\n",
    "        for i in range(width):\n",
    "            beam_best[i][1] = idx[0][i]\n",
    "            prob_best[i] = val[0][i].exp() #since prob log,softmax from -10 to -12\n",
    "#             best_output[i] = idx[0][i]\n",
    "        \n",
    "        for t in range(2, max_len):\n",
    "            cnt = 0\n",
    "            for i in range(width):\n",
    "                curr_prob = prob_best[i]\n",
    "                #\n",
    "                hidden = self.encoder(src, src_len)[1][:self.decoder.num_layers] \n",
    "                \n",
    "                #feed the entire vector for the training process\n",
    "                for j in range(t):\n",
    "                    temp = beam_best[i][j].reshape(1).long()\n",
    "                    new_output, hidden, _ = self.decoder(\n",
    "                            temp, hidden, encoder_output)\n",
    "                \n",
    "#                 new_output.data = new_output.data.exp() * curr_prob \n",
    "                val1, idx1 = new_output.data.topk(k = width, dim = 1)\n",
    "                \n",
    "                for j in range(width):\n",
    "                    beam_options[cnt] = beam_best[i]\n",
    "                    beam_options[cnt][t] = idx1[0][j]\n",
    "                    prob_options[cnt] = val1[0][j].exp() * curr_prob\n",
    "#                     outputs[cnt] = idx1[0][j]\n",
    "                    cnt += 1\n",
    "            \n",
    "#             print('prob_options' + str(prob_options))\n",
    "            topVal, topInx = prob_options.topk(k = width, dim = 0)\n",
    "#             print()\n",
    "#             print('topVal ' + str(topVal))\n",
    "#             print('topInx ' + str(topInx))\n",
    "#         \n",
    "            for j in range(topInx.size(0)):\n",
    "                prob_best[j] = topVal[j]\n",
    "                beam_best[j] = beam_options[topInx[j]]  \n",
    "#                 best_output[j] = outputs[topInx[j]]\n",
    "#         print('prob_best ' + str(prob_best))\n",
    "\n",
    "            \n",
    "            \n",
    "        best_index = prob_best.max(0)[1]\n",
    "#         print('best_inx' + str(best_index))\n",
    "#         print('beam_option best' + str(beam_options[best_index]))\n",
    "        return beam_options[best_index].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 3\n",
    "net = BeamSearch(net.encoder,net.decoder, width).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ans[0.000e+00 1.210e+02 3.042e+03 1.230e+02 9.800e+01 9.800e+01 1.760e+02\n",
      " 1.472e+03 7.374e+03 4.000e+01 4.000e+00 3.720e+02 2.400e+01 1.000e+00\n",
      " 1.000e+00 2.400e+01 1.000e+00 1.000e+00 2.400e+01 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 2.400e+01 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 2.400e+01 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 2.400e+01 1.000e+00 1.000e+00\n",
      " 1.000e+00]\n",
      "INPUT:\n",
      "<sos> 朱邦造 说 , 我们 注意到 有关 报道 . <eos>\n",
      "REF:\n",
      "<sos> zhu bangzao said we have taken note of the report . <eos>\n",
      "PREDICTION:\n",
      "<sos> zhu bangzao said we we have taken note of the report . <eos>\n",
      "------\n",
      "ans[0.000e+00 2.500e+01 1.500e+01 8.270e+02 1.500e+01 3.100e+01 7.460e+02\n",
      " 4.130e+02 4.600e+01 4.160e+02 2.123e+03 2.400e+01 1.000e+00 2.400e+01\n",
      " 1.000e+00 1.000e+00 2.400e+01 1.000e+00 1.000e+00 2.400e+01 1.000e+00\n",
      " 1.000e+00 2.400e+01 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 2.400e+01 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 2.400e+01\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 2.400e+01\n",
      " 1.000e+00 1.000e+00 2.400e+01 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00]\n",
      "INPUT:\n",
      "<sos> 但是 墨西哥 的 表态 也是 积极 的 . <eos>\n",
      "REF:\n",
      "<sos> mexico , however , has also expressed a positive attitude . <eos>\n",
      "PREDICTION:\n",
      "<sos> mexico , however , has also expressed a positive attitude . <eos>\n",
      "------\n",
      "ans[0.000e+00 4.000e+00 2.847e+03 4.000e+01 4.000e+00 7.855e+03 1.684e+03\n",
      " 2.960e+02 7.856e+03 1.684e+03 1.684e+03 2.400e+01 1.000e+00 2.400e+01\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00]\n",
      "INPUT:\n",
      "<sos> 李明全 , 蒋仕葆 双 机 迅速 起飞 . <eos>\n",
      "REF:\n",
      "<sos> the aircraft of lu mingquan and jiang shibao rapidly took off . <eos>\n",
      "PREDICTION:\n",
      "<sos> the aircraft of the mingquan rapidly jiang shibao rapidly rapidly . <eos>\n",
      "------\n",
      "ans[0.000e+00 6.200e+01 1.500e+01 4.000e+00 6.340e+03 2.740e+02 4.152e+03\n",
      " 4.152e+03 2.400e+01 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00]\n",
      "INPUT:\n",
      "<sos> 因此 共和党 人 不能 否决 这项 议案 . <eos>\n",
      "REF:\n",
      "<sos> therefore the republicans cannot veto the bill . <eos>\n",
      "PREDICTION:\n",
      "<sos> therefore , the republicans cannot veto veto . <eos>\n",
      "------\n",
      "ans[0.000e+00 4.000e+00 1.739e+03 4.000e+01 1.526e+03 2.564e+03 9.900e+01\n",
      " 8.400e+01 1.442e+03 6.910e+02 1.000e+01 4.000e+00 1.438e+03 1.439e+03\n",
      " 1.440e+03 1.441e+03 2.400e+01 1.000e+00 2.400e+01 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00]\n",
      "INPUT:\n",
      "<sos> 审计 部 的 影响力 在 大亚湾 随处可见 . <eos>\n",
      "REF:\n",
      "<sos> the influence of the auditing department can be seen everywhere in the daya bay power station . <eos>\n",
      "PREDICTION:\n",
      "<sos> the influence of auditing department can be seen everywhere in the daya bay power station . <eos>\n",
      "------\n",
      "ans[0.000e+00 4.000e+00 2.250e+02 2.260e+02 4.250e+02 4.300e+01 2.053e+03\n",
      " 2.710e+02 1.300e+03 2.220e+02 5.670e+03 4.000e+01 8.436e+03 2.400e+01\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00]\n",
      "INPUT:\n",
      "<sos> 各级 领导干部 一定 要 增强 忧患 意识 . <eos>\n",
      "REF:\n",
      "<sos> the leading cadres at all levels must enhance their awareness of hardship . <eos>\n",
      "PREDICTION:\n",
      "<sos> the leading cadres at all levels must enhance their awareness of hardship . <eos>\n",
      "------\n",
      "ans[0.000e+00 6.500e+02 3.824e+03 4.000e+00 2.875e+03 4.000e+01 1.181e+03\n",
      " 1.200e+01 3.210e+02 2.610e+02 5.247e+03 1.182e+03 1.182e+03 1.000e+00\n",
      " 5.247e+03 1.182e+03 1.182e+03 1.182e+03 1.182e+03 1.182e+03 1.000e+00\n",
      " 1.182e+03 1.000e+00 1.182e+03 1.000e+00 1.182e+03 1.182e+03 1.000e+00\n",
      " 1.182e+03 1.000e+00 1.182e+03 1.000e+00 1.182e+03 1.000e+00 1.182e+03\n",
      " 1.000e+00 1.182e+03 1.000e+00 1.182e+03 1.182e+03 1.000e+00 1.182e+03\n",
      " 1.000e+00 1.182e+03 1.000e+00 1.182e+03 1.000e+00 1.182e+03 1.000e+00\n",
      " 1.182e+03]\n",
      "INPUT:\n",
      "<sos> 祝 菲律宾 共和国 繁荣昌盛 , 人民 幸福 ! <eos>\n",
      "REF:\n",
      "<sos> i wish the republic of the philippines prosperity and its people happiness ! <eos>\n",
      "PREDICTION:\n",
      "<sos> i wish the republic of prosperity and its people happiness ! ! <eos>\n",
      "------\n",
      "ans[0.000e+00 4.100e+01 2.130e+02 1.154e+03 4.100e+01 2.600e+01 4.600e+01\n",
      " 1.550e+03 9.900e+02 4.891e+03 2.400e+01 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00]\n",
      "INPUT:\n",
      "<sos> \" 台独 \" 是 一 条 死路 . <eos>\n",
      "REF:\n",
      "<sos> \" taiwan independence \" is a road toward destruction . <eos>\n",
      "PREDICTION:\n",
      "<sos> \" taiwan independence \" is a road toward destruction . <eos>\n",
      "------\n",
      "ans[0.000e+00 3.019e+03 7.741e+03 1.230e+02 3.000e+01 1.400e+01 1.200e+01\n",
      " 3.090e+03 2.270e+02 1.332e+03 4.693e+03 2.400e+01 1.000e+00 2.400e+01\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 2.400e+01 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 2.400e+01 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 2.400e+01 1.000e+00 1.000e+00 2.400e+01 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00]\n",
      "INPUT:\n",
      "<sos> 尉健行 说 , 中韩 两国 是 近邻 . <eos>\n",
      "REF:\n",
      "<sos> wei jianxing said that china and rok are close neighbors . <eos>\n",
      "PREDICTION:\n",
      "<sos> wei jianxing said that china and rok are close neighbors . <eos>\n",
      "------\n",
      "ans[0.000e+00 1.420e+02 8.100e+02 3.425e+03 4.000e+00 4.786e+03 6.320e+02\n",
      " 6.330e+02 4.000e+01 4.000e+00 2.813e+03 4.000e+01 5.270e+02 5.280e+02\n",
      " 1.050e+02 1.400e+01 1.200e+01 8.752e+03 2.400e+01 1.000e+00 2.400e+01\n",
      " 1.000e+00 1.000e+00 1.000e+00 2.400e+01 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
      " 1.000e+00]\n",
      "INPUT:\n",
      "<sos> 今年 适逢 中 萨 建交 25 周年 . <eos>\n",
      "REF:\n",
      "<sos> this year marks the 25 th anniversary of the establishment of diplomatic ties between china and samoa . <eos>\n",
      "PREDICTION:\n",
      "<sos> this year marks the 25 th anniversary of the establishment of diplomatic ties between china and samoa . <eos>\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "# from utils import bleu\n",
    "pairs.sort(key=lambda x: len(x[0].split()))\n",
    "\n",
    "for index_sample in range(2000,2010):\n",
    "    input_batches, input_lengths,\\\n",
    "        target_batches, target_lengths = random_batch(test_src,test_tgt,pairs,1,index_sample)\n",
    "\n",
    "    # import bleu\n",
    "    for test_idx in range(1):\n",
    "    #     pred = net(input_batches[:,test_idx].reshape(input_lengths[0].item(),1),input_lengths[0].reshape(1), 5)\n",
    "    #     print(\"input_batches \" + str(input_batches.size()))\n",
    "    #     print(\"input_length \" + str(input_lengths))\n",
    "    #     print(\"target_batches \" + str(target_batches.size()))\n",
    "    #     print(\"target_lengths \" + str(target_lengths))\n",
    "        pred = net(input_batches[:,test_idx].reshape(input_lengths[0].item(),1), input_lengths[0].reshape(1), width, MAX_LENGTH)\n",
    "\n",
    "        print('ans' + str(pred))\n",
    "        inp = ' '.join([src.idx2w[t] for t in input_batches[:,test_idx].cpu().numpy()])\n",
    "        mt = ' '.join([tgt.idx2w[t] for t in pred if t!= PAD_idx])\n",
    "        idx = mt.find('<eos>')\n",
    "#         print(idx)\n",
    "        mt = mt[:idx+5]\n",
    "        ref = ' '.join([tgt.idx2w[t] for t in target_batches[:,test_idx].cpu().numpy() if t != PAD_idx])\n",
    "        print('INPUT:\\n' + inp)\n",
    "        print('REF:\\n' + ref)\n",
    "        print('PREDICTION:\\n' + mt)\n",
    "        #print('BLEU = %f' % bleu([mt],[[ref]],4))\n",
    "        print(\"------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def modified_precision(candidate, references, n):\n",
    "    count = 0\n",
    "    match_num = 0\n",
    "    len_c = len(candidate)\n",
    "    \n",
    "    for i in range(len_c):\n",
    "        ref_temp = []\n",
    "        for reference in references:\n",
    "            for k in range(len(reference)):\n",
    "                ref_sentence = reference[k]\n",
    "                words_ref_sentence = ref_sentence.strip()\n",
    "                words_ref_sentence = words_ref_sentence.split()\n",
    "                num_max = len(words_ref_sentence) - n + 1\n",
    "                ngram_temp = {}\n",
    "                for j in range(num_max):\n",
    "                    ngram = ' '.join(words_ref_sentence[j:j+n])\n",
    "                    ngram = ngram.lower()\n",
    "                    if ngram in ngram_temp.keys():\n",
    "                        ngram_temp[ngram] += 1\n",
    "                    else:\n",
    "                        ngram_temp[ngram] = 1\n",
    "                ref_temp.append(ngram_temp)\n",
    "            \n",
    "        cand_sentence = candidate[i]\n",
    "        words_cand = cand_sentence.strip()\n",
    "        words_cand = words_cand.split()\n",
    "        num_max_cand = len(words_cand) - n + 1\n",
    "        cand_temp = {}\n",
    "        for j in range(num_max_cand):\n",
    "            ngram = ' '.join(words_cand[j:j+n])\n",
    "            ngram = ngram.lower()\n",
    "            if ngram in cand_temp.keys():\n",
    "                cand_temp[ngram] += 1\n",
    "            else:\n",
    "                cand_temp[ngram] = 1\n",
    "        count += num_max_cand\n",
    "        match_num += match_counts(ref_temp, cand_temp)\n",
    "    if match_num != 0:\n",
    "        p = 1. * match_num / count\n",
    "    else:\n",
    "        p = 0\n",
    "    return p\n",
    "\n",
    "def match_counts(ref_counts, cand_temp):\n",
    "    num = 0\n",
    "    for ngram in cand_temp.keys():\n",
    "        count = cand_temp[ngram]\n",
    "        max_ref = 0\n",
    "        for ref in ref_counts:\n",
    "            if ngram in ref:\n",
    "                max_ref = max(max_ref, ref[ngram])\n",
    "        count = min(max_ref, count)\n",
    "        num = num + count\n",
    "    return num\n",
    "\n",
    "def brevity_penalty(candidate, references):\n",
    "    len_c = len(candidate)\n",
    "    r = 0\n",
    "    c = 0\n",
    "    for i in range(len_c):\n",
    "        ref_lens = []\n",
    "        for reference in references:\n",
    "            for k in range(len(reference)):\n",
    "                ref_sentence = reference[k]\n",
    "                words_ref_sentence = ref_sentence.strip()\n",
    "                words_ref_sentence = words_ref_sentence.split()\n",
    "                ref_lens.append(len(words_ref_sentence))\n",
    "        cand_sentence = candidate[i]\n",
    "        words_cand = cand_sentence.strip().split()\n",
    "        init_len_diff = abs(len(words_cand)-ref_lens[0])\n",
    "        best = ref_lens[0]\n",
    "        for num in ref_lens:\n",
    "            if (abs(len(words_cand)-num)) < init_len_diff:\n",
    "                init_len_diff = abs(len(words_cand) - num)\n",
    "                best = num\n",
    "        r = r + best\n",
    "        c = c + len(words_cand)\n",
    "    if c > r:\n",
    "        bp = 1\n",
    "    else:\n",
    "        bp = math.exp(1 - 1. * r / c)\n",
    "    return bp\n",
    "\n",
    "def bleu(candidate, references, n):\n",
    "    # n is the maximum length of each ngram you set \n",
    "    weight = 1./n\n",
    "    temp = 0\n",
    "    for i in range(n):\n",
    "        p = modified_precision(candidate, references, i + 1)\n",
    "        temp += math.log(p) * weight if p != 0 else 0\n",
    "    temp = math.exp(temp)\n",
    "    return brevity_penalty(candidate, references) * temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 finished :)0.7168577677102913\n",
      "2 finished :)0.7487742968991468\n",
      "3 finished :)0.7466207080823652\n",
      "4 finished :)0.796394968709534\n",
      "5 finished :)0.793056527178793\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "beam_widths = [1,2,3,4,5]\n",
    "pairs.sort(key=lambda x: len(x[0].split()))\n",
    "blue_score = []\n",
    "for i in beam_widths:\n",
    "#     net = BeamSearch(net.encoder,net.decoder, i).cuda()\n",
    "    for index_sample in range(2500,2510):\n",
    "        input_batches, input_lengths,\\\n",
    "            target_batches, target_lengths = random_batch(test_src,test_tgt,pairs,1,index_sample)\n",
    "\n",
    "        # import bleu\n",
    "        for test_idx in range(1):\n",
    "            pred = net(input_batches[:,test_idx].reshape(input_lengths[0].item(),1), input_lengths[0].reshape(1), i, MAX_LENGTH)\n",
    "    #         print('ans' + str(pred))\n",
    "            inp = ' '.join([src.idx2w[t] for t in input_batches[:,test_idx].cpu().numpy()])\n",
    "            mt = ' '.join([tgt.idx2w[t] for t in pred if t!= PAD_idx])\n",
    "            idx = mt.find('<eos>')\n",
    "            mt = mt[:idx+5]\n",
    "            ref = ' '.join([tgt.idx2w[t] for t in target_batches[:,test_idx].cpu().numpy() if t != PAD_idx])\n",
    "    #         print('INPUT:\\n' + inp)\n",
    "    #         print('REF:\\n' + ref)\n",
    "    #         print('PREDICTION:\\n' + mt)\n",
    "            blue_score.append(bleu([mt],[[ref]],4))\n",
    "    #         print(\"------\")\n",
    "    print(str(i) + \" finished :)\" + str(np.mean(blue_score[i*10-10:i*10])))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
