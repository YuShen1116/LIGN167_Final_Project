{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils import clip_grad_norm_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearch(nn.Module):\n",
    "    '''\n",
    "    Implement BeamSearch for testing\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, src, src_len, tgt, tgt_len, width, upper_limit):\n",
    "        max_len = tgt.size(0)\n",
    "        vocab_size = self.decoder.dim_output\n",
    "#         outputs = Variable(torch.zeros(max_len, width, vocab_size)).cuda()\n",
    "        encoder_output, hidden = self.encoder(src, src_len)\n",
    "        hidden = hidden[:self.decoder.num_layers]\n",
    "        \n",
    "        #output stores (probability, vector)\n",
    "        output = Variable(torch.zeros(width, tgt.size(0))).cuda()\n",
    "        upperBound = Variable(torch.zeros(upper_limit, tgt.size(0))).cude()\n",
    "        \n",
    "        prob = torch.tensor(0)\n",
    "        #start with one word with probability of 1\n",
    "        output[0][0] = 1\n",
    "        output[0][1] = tgt.data[0, :]\n",
    "        \n",
    "        for t in range(1, max_len):\n",
    "            idx = 0\n",
    "            for i in range(width):\n",
    "                #probability of previous vector\n",
    "                prob = output[i][1].data\n",
    "                \n",
    "                output[i][1], hidden, attn_weights = self.decoder(\n",
    "                        output[i][1], hidden, encoder_output)\n",
    "                \n",
    "                #update at most upper_limit number of possible vector\n",
    "                for item in output[i][1].data:\n",
    "                    upperBound[idx][1] = item\n",
    "                    upperBound[idx][0] = item.data * (prob + 1e-9)\n",
    "                    idx += 1\n",
    "                    if idx >= upper_limit:\n",
    "                        break\n",
    "                \n",
    "                #keep the width size\n",
    "                val, idx = torch.topk(upperBound, k = width, dim = 0)\n",
    "                sorted_idx, new_idx = torch.sort(idx, dim=-1)\n",
    "                output = torch.gather(val, dim=-1, index=new_idx)    \n",
    "\n",
    "        return output.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.rand(2, 3, 5)\n",
    "# val, idx = torch.topk(x, k=2, dim=-1) # k greatest elements\n",
    "# sorted_idx, new_idx = torch.sort(idx, dim=-1)\n",
    "# val = torch.gather(val, dim=-1, index=new_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
