{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearch:\n",
    "    '''\n",
    "    Basic steps for beam search:\n",
    "    1. initialize a list of empty beam path with size = width -- k\n",
    "    2. choose first possible N candidates (N >= k), construct N * k probability paths\n",
    "    3. score on beam paths and take first k prob_path as beam paths for next step\n",
    "    4. terminate when decoding finish, otherwise go back to step 2\n",
    "    '''\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vocab = {\n",
    "            0: 'a',\n",
    "            1: 'b',\n",
    "            2: 'c',\n",
    "            3: 'd',\n",
    "            4: 'e',\n",
    "            5: 'SOS',\n",
    "            6: 'EOS'\n",
    "        }\n",
    "        self.vocab_pair = dict([(v,k) for k,v in self.vocab.items()])\n",
    "        self.vocab_size = len(self.vocab.items())\n",
    "    \n",
    "    def softmax(x):\n",
    "        '''\n",
    "        Compute softmax values for each sets of scores in x.\n",
    "        '''\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum()\n",
    "    \n",
    "    def reduce_mul(l):\n",
    "        out = 1.0\n",
    "        for x in l:\n",
    "            out *= x\n",
    "        return out\n",
    "    \n",
    "    def check_all_done(seqs):\n",
    "        for seq in seqs:\n",
    "            if not seq[-1]:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def decode_step(encoder_context, input_seq):    \n",
    "        #encoder_context contains infortaion of encoder\n",
    "        #ouput_step contains the words' probability\n",
    "        #these two varibles should be generated by seq2seq model\n",
    "        words_prob = [random.random() for _ in range(vocab_size)]\n",
    "        #downvote BOS\n",
    "        words_prob[vocab_pair['BOS']] = 0.0\n",
    "        words_prob = softmax(words_prob)\n",
    "        ouput_step = [(idx,prob) for idx,prob in enumerate(words_prob)]        \n",
    "        ouput_step = sorted(ouput_step, key=lambda x: x[1], reverse=True)\n",
    "        return ouput_step\n",
    "\n",
    "    def beam_search_step(encoder_context, top_seqs, k):       \n",
    "        '''\n",
    "        input: [[word,word],[word,word],[word,word]]\n",
    "        output: [[word,word,word],[word,word,word],[word,word,word]]\n",
    "        '''\n",
    "        all_seqs = []\n",
    "        for seq in top_seqs:\n",
    "            seq_score = reduce_mul([_score for _,_score in seq])\n",
    "            if seq[-1][0] == vocab_pair['SOS']:\n",
    "                all_seqs.append((seq, seq_score, True))\n",
    "                continue\n",
    "            #get current step using encoder_context & seq\n",
    "            current_step = decode_step(encoder_context, seq)\n",
    "            for i,word in enumerate(current_step):    \n",
    "                if i >= k:\n",
    "                    break\n",
    "                word_index = word[0]\n",
    "                word_score = word[1]   \n",
    "                score = seq_score * word_score\n",
    "                rs_seq = seq + [word]\n",
    "                done = (word_index == vocab_pair['EOS'])            \n",
    "                all_seqs.append((rs_seq, score, done))            \n",
    "        all_seqs = sorted(all_seqs, key = lambda seq: seq[1], reverse=True)        \n",
    "        topk_seqs = [seq for seq,_,_ in all_seqs[:k]]\n",
    "        all_done = check_all_done(topk_seqs)\n",
    "        return topk_seqs, all_done\n",
    "\n",
    "    def beam_search(encoder_context, beam_width, max_len):\n",
    "        #START\n",
    "        top_seqs = [[(vocab_pair['SOS'],1.0)]]\n",
    "        #loop\n",
    "        for _ in range(max_len):        \n",
    "            top_seqs, all_done = beam_search_step(encoder_context, top_seqs, beam_width)\n",
    "            if all_done:            \n",
    "                break        \n",
    "        return top_seqs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Beam_Search = BeamSearch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
