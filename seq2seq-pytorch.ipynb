{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "\n",
    "# import basic lib\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import string\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "from io import open\n",
    "\n",
    "# import pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "# import helper function\n",
    "import masked_cross_entropy\n",
    "from infer_eval import bleu\n",
    "\n",
    "# check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_idx = 0\n",
    "EOS_idx = 1\n",
    "UNK_idx = 2\n",
    "PAD_idx = 3\n",
    "\n",
    "USE_CUDA = True\n",
    "\n",
    "class Preprocessor:\n",
    "    '''\n",
    "    class for preprocessing\n",
    "    '''\n",
    "    def __init__(self, name):\n",
    "        '''\n",
    "        initialize vocab and counter\n",
    "        '''\n",
    "        self.name = name\n",
    "        self.w2idx = {\"<sos>\" : 0, \"<eos>\" : 1, \"<unk>\" : 2, \"<pad>\" : 3}\n",
    "        self.counter = {}\n",
    "        self.idx2w = {0: \"<sos>\", 1: \"<eos>\", 2:\"<unk>\", 3:\"<pad>\"}\n",
    "        self.num = 4\n",
    "\n",
    "    def SentenceAdder(self, sentence):\n",
    "        '''\n",
    "        Add a sentence to dataset\n",
    "        '''\n",
    "        for word in sentence.split(' '):\n",
    "            self.WordAdder(word)\n",
    "\n",
    "    def WordAdder(self, word):\n",
    "        '''\n",
    "        Add single word to dataset and update vocab and counter\n",
    "        '''\n",
    "        if word in self.w2idx:\n",
    "            self.counter[word] += 1\n",
    "        else:\n",
    "            self.w2idx[word] = self.num\n",
    "            self.counter[word] = 1\n",
    "            self.idx2w[self.num] = word\n",
    "            self.num += 1\n",
    "            \n",
    "    def trim(self, min_count=1):\n",
    "        '''\n",
    "        Trim to remove non-frequent word\n",
    "        '''\n",
    "        keep = []\n",
    "        for k, v in self.counter.items():\n",
    "            if v >= min_count: keep.append(k)\n",
    "        print(self.name+':')\n",
    "        print('Total words', len(self.w2idx))\n",
    "        print('After Trimming', len(keep))\n",
    "        print('Keep Ratio %', 100 * len(keep) / len(self.w2idx))\n",
    "        self.w2idx = {\"<sos>\" : 0, \"<eos>\" : 1, \"<unk>\" : 2, \"<pad>\" : 3}\n",
    "        self.counter = {}\n",
    "        self.idx2w = {0: \"<sos>\", 1: \"<eos>\", 2:\"<unk>\", 3:\"<pad>\"}\n",
    "        self.num = 4\n",
    "        for w in keep:\n",
    "            self.WordAdder(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Uni2Ascii(s):\n",
    "    '''\n",
    "    transfer from unicode to ascii\n",
    "    '''\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                    if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def StrCleaner(s):\n",
    "    '''\n",
    "    trim, delete non-letter and lowercase string\n",
    "    '''\n",
    "    s = Uni2Ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def DataReader(path, lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open(path, encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    #pairs = [[StrCleaner(s) for s in l.split('<------>')] for l in lines]\n",
    "    pairs = [[s.lower() for s in l.split('<------>')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Preprocessor(lang2)\n",
    "        output_lang = Preprocessor(lang1)\n",
    "    else:\n",
    "        input_lang = Preprocessor(lang1)\n",
    "        output_lang = Preprocessor(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_LENGTH = 3\n",
    "MAX_LENGTH = 50\n",
    "\n",
    "def filterPair(p):\n",
    "    '''\n",
    "    Filter to get expected pairs with specific length\n",
    "    '''\n",
    "    return MIN_LENGTH <= len(p[0].split(' ')) <= MAX_LENGTH and \\\n",
    "        MIN_LENGTH <= len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 90000 sentence pairs\n",
      "Trimmed to 77083 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "chinese 46064\n",
      "english 30499\n",
      "chinese:\n",
      "Total words 46064\n",
      "After Trimming 46060\n",
      "Keep Ratio % 99.99131642931573\n",
      "english:\n",
      "Total words 30499\n",
      "After Trimming 30495\n",
      "Keep Ratio % 99.9868848158956\n",
      "['周恩来 同志 亲自 制定 中国 援外 八 原则 , 至今 仍是 我们 开展 国际 经济 技术 合作 的 重要 指南 .', \"comrade zhou enlai worked out eight principles on supporting foreign countries , which thus far still serve as an important guidance for developing international economic and technological cooperation . comrade xiaoping was the chief architect of china 's reform and opening up .\"]\n"
     ]
    }
   ],
   "source": [
    "def prepareData(path, lang1, lang2, reverse=True):\n",
    "    input_lang, output_lang, pairs = DataReader(path, lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.SentenceAdder(pair[0])\n",
    "        output_lang.SentenceAdder(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.num)\n",
    "    print(output_lang.name, output_lang.num)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "src, tgt, pairs = prepareData('data/train.txt', 'english', 'chinese')\n",
    "src.trim()\n",
    "tgt.trim()\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 10000 sentence pairs\n",
      "Trimmed to 8572 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "chinese 17294\n",
      "english 13049\n",
      "chinese:\n",
      "Total words 17294\n",
      "After Trimming 17290\n",
      "Keep Ratio % 99.9768705909564\n",
      "english:\n",
      "Total words 13049\n",
      "After Trimming 13045\n",
      "Keep Ratio % 99.96934631006208\n",
      "['加快 国际 通道 建设 .', 'we should step up the building of international communications .']\n"
     ]
    }
   ],
   "source": [
    "test_src, test_tgt, test_pairs = prepareData('data/test.txt', 'english', 'chinese')\n",
    "test_src.trim()\n",
    "test_tgt.trim()\n",
    "print(random.choice(test_pairs))\n",
    "test_src.w2idx, test_src.idx2w, test_src.num = src.w2idx, src.idx2w, src.num\n",
    "test_tgt.w2idx, test_tgt.idx2w, test_tgt.num = tgt.w2idx, tgt.idx2w, tgt.num\n",
    "test_pairs.sort(key=lambda x: len(x[0].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2idx(preprocessor, sentence):\n",
    "    '''\n",
    "    Read sentence and translate into word index plus eos\n",
    "    '''\n",
    "    return [SOS_idx] + [preprocessor.w2idx[w] if w in preprocessor.w2idx \\\n",
    "            else UNK_idx for w in sentence.split(' ')] + [EOS_idx]\n",
    "\n",
    "def pad(seq, max_len):\n",
    "    '''\n",
    "    Add padding to sentence with different length\n",
    "    '''\n",
    "    seq += [PAD_idx for i in range(max_len - len(seq))]\n",
    "    return seq\n",
    "\n",
    "def random_batch(src, tgt, pairs, batch_size, batch_idx):\n",
    "    '''\n",
    "    Randomly generate batch data\n",
    "    '''\n",
    "    inputs, target = [], []\n",
    "    \n",
    "    # Choose batch\n",
    "    for s in pairs[batch_idx*batch_size:(batch_idx+1)*batch_size]:\n",
    "        inputs.append(sentence2idx(src, s[0]))\n",
    "        target.append(sentence2idx(tgt, s[1]))\n",
    "        \n",
    "    # Sort by length\n",
    "    seq_pairs = sorted(zip(inputs, target), key=lambda p: len(p[0]), reverse=True)\n",
    "    inputs, target = zip(*seq_pairs)\n",
    "    \n",
    "    # Obtain length of each sentence and pad\n",
    "    input_lens = [len(s) for s in inputs]\n",
    "    input_max = max(input_lens)\n",
    "    input_padded = [pad(s, input_max) for s in inputs]\n",
    "    target_lens = [len(s) for s in target]\n",
    "    target_max = max(target_lens)\n",
    "    target_padded = [pad(s, target_max) for s in target]\n",
    "\n",
    "    # Create Variable\n",
    "    if USE_CUDA:\n",
    "        input_vars = Variable(torch.LongTensor(input_padded).cuda()).transpose(0, 1)\n",
    "        input_lens = Variable(torch.LongTensor(input_lens).cuda())\n",
    "        target_vars = Variable(torch.LongTensor(target_padded).cuda()).transpose(0, 1)\n",
    "        target_lens = Variable(torch.LongTensor(target_lens).cuda())\n",
    "    else:\n",
    "        input_vars = Variable(torch.LongTensor(input_padded)).transpose(0, 1)\n",
    "        input_lens = Variable(torch.LongTensor(input_lens))\n",
    "        target_vars = Variable(torch.LongTensor(target_padded)).transpose(0, 1)\n",
    "        target_lens = Variable(torch.LongTensor(target_lens))\n",
    "\n",
    "    return input_vars, input_lens, target_vars, target_lens\n",
    "\n",
    "def user_input(inputs, src):\n",
    "    inp_list = [sentence2idx(src, inputs)]\n",
    "    input_lens = [len(s) for s in inp_list]\n",
    "    input_max = max(input_lens)\n",
    "    input_padded = [pad(s, input_max) for s in inp_list]\n",
    "    if USE_CUDA:\n",
    "        input_vars = Variable(torch.LongTensor(input_padded).cuda()).transpose(0, 1)\n",
    "        input_lens = Variable(torch.LongTensor(input_lens).cuda())\n",
    "    else:\n",
    "        input_vars = Variable(torch.LongTensor(input_padded)).transpose(0, 1)\n",
    "        input_lens = Variable(torch.LongTensor(input_lens))\n",
    "    return input_vars, input_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    '''\n",
    "    Define encoder and forward process\n",
    "    '''\n",
    "    def __init__(self, dim_input, dim_embed, dim_hidden, num_layers, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dim_input = dim_input\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.dim_embed = dim_embed\n",
    "        self.embed = nn.Embedding(dim_input, dim_embed)\n",
    "        self.cell = nn.GRU(dim_embed, dim_hidden, \n",
    "                          num_layers, dropout=dropout, \n",
    "                          bidirectional=True)\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        if USE_CUDA:\n",
    "            return Variable(torch.zeros(self.n_layers, 1, self.hidden_size).cuda())\n",
    "        else:\n",
    "            return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))\n",
    "        \n",
    "    def forward(self, inputs, inputs_lens, hidden=None):\n",
    "        '''\n",
    "        We need to sum the outputs since bi-diretional is used\n",
    "        '''\n",
    "        embedded = self.embed(inputs)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, inputs_lens)\n",
    "        outputs, hidden = self.cell(packed, hidden)\n",
    "        outputs, output_lengths = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        outputs = outputs[:, :, :self.dim_hidden] + \\\n",
    "                    outputs[:, :, self.dim_hidden:]\n",
    "        return outputs, hidden\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    '''\n",
    "    Define attention mechanism\n",
    "    '''\n",
    "    def __init__(self, dim_hidden):\n",
    "        super(Attention, self).__init__()\n",
    "        self.dim_hidden = dim_hidden\n",
    "        # 2*dim_hidden is needed since bi-direction is used\n",
    "        self.attn = nn.Linear(2*self.dim_hidden, dim_hidden)\n",
    "        self.v = nn.Parameter(torch.rand(dim_hidden))\n",
    "        stdv = 1. / math.sqrt(self.v.size(0))\n",
    "        self.v.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        timestep = encoder_outputs.size(0)\n",
    "        h = hidden.repeat(timestep, 1, 1).transpose(0, 1)\n",
    "        encoder_outputs = encoder_outputs.transpose(0, 1)\n",
    "        scores = self.score(h, encoder_outputs)\n",
    "        return F.relu(scores).unsqueeze(1)\n",
    "\n",
    "    def score(self, hidden, encoder_outputs):\n",
    "        e = F.softmax(self.attn(torch.cat([hidden, encoder_outputs], 2)),dim=1)\n",
    "        e = e.transpose(1, 2)\n",
    "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)\n",
    "        e = torch.bmm(v, e)\n",
    "        return e.squeeze(1)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    '''\n",
    "    Define decoder with attention\n",
    "    '''\n",
    "    def __init__(self, dim_embed, dim_hidden, dim_output, num_layers, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dim_embed = dim_embed\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.dim_output = dim_output\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embed = nn.Embedding(dim_output, dim_embed)\n",
    "        self.dropout = nn.Dropout(dropout, inplace=True)\n",
    "        self.attention = Attention(dim_hidden)\n",
    "        self.cell = nn.GRU(dim_hidden + dim_embed, dim_hidden,\n",
    "                          num_layers, dropout=dropout)\n",
    "        self.out = nn.Linear(2*dim_hidden, dim_output)\n",
    "\n",
    "    def forward(self, inputs, last_hidden, encoder_outputs):\n",
    "        \n",
    "        embedded = self.embed(inputs).unsqueeze(0)  # (1,B,N)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        attn_weights = self.attention(last_hidden[-1], encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))  # (B,1,N)\n",
    "        context = context.transpose(0, 1)  # (1,B,N)\n",
    "        \n",
    "        rnn_input = torch.cat([embedded, context], 2)\n",
    "        output, hidden = self.cell(rnn_input, last_hidden)\n",
    "        output = output.squeeze(0)  # (1,B,N) -> (B,N)\n",
    "        context = context.squeeze(0)\n",
    "        output = self.out(torch.cat([output, context], 1))\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, src_len, tgt, tgt_len, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.size(1)\n",
    "        max_len = tgt.size(0)\n",
    "        vocab_size = self.decoder.dim_output\n",
    "        if USE_CUDA:\n",
    "            outputs = Variable(torch.zeros(max_len, batch_size, vocab_size).cuda())\n",
    "        else:\n",
    "            outputs = Variable(torch.zeros(max_len, batch_size, vocab_size))\n",
    "        encoder_output, hidden = self.encoder(src, src_len)\n",
    "        hidden = hidden[:self.decoder.num_layers]\n",
    "        # Put <sos> at first position\n",
    "        if USE_CUDA:\n",
    "            output = Variable(tgt.data[0, :].cuda())\n",
    "        else:\n",
    "            output = Variable(tgt.data[0, :])\n",
    "        for t in range(1, max_len):\n",
    "            output, hidden, attn_weights = self.decoder(\n",
    "                    output, hidden, encoder_output)\n",
    "            outputs[t] = output\n",
    "            # Randomly choose whether to use teacher force or not\n",
    "            is_teacher = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.data.max(1)[1]\n",
    "            if USE_CUDA:\n",
    "                output = Variable(tgt.data[t].cuda() if is_teacher else top1.cuda())\n",
    "            else:\n",
    "                output = Variable(tgt.data[t] if is_teacher else top1)\n",
    "        return outputs\n",
    "    \n",
    "    def inference(self, src, src_len, max_len = MAX_LENGTH):\n",
    "        pred_idx = []\n",
    "        batch_size = src.size(1)\n",
    "        vocab_size = self.decoder.dim_output\n",
    "        if USE_CUDA:\n",
    "            outputs = Variable(torch.zeros(max_len, batch_size, vocab_size).cuda())\n",
    "        else:\n",
    "            outputs = Variable(torch.zeros(max_len, batch_size, vocab_size))\n",
    "        \n",
    "        encoder_output, hidden = self.encoder(src, src_len)\n",
    "        hidden = hidden[:self.decoder.num_layers]\n",
    "        # Put <sos> at first position\n",
    "        if USE_CUDA:\n",
    "            output = Variable(src.data[0, :].cuda())\n",
    "        else:\n",
    "            output = Variable(src.data[0, :])\n",
    "        pred_idx.append(SOS_idx)\n",
    "        for t in range(1, max_len):\n",
    "            output, hidden, attn_weights = self.decoder(\n",
    "                    output, hidden, encoder_output)\n",
    "            outputs[t] = output\n",
    "            top1 = output.data.max(1)[1]\n",
    "            pred_idx.append(top1.item())\n",
    "            if USE_CUDA:\n",
    "                output = Variable(top1.cuda())\n",
    "            else:\n",
    "                output = Variable(top1)\n",
    "            if top1 == EOS_idx: break\n",
    "        return outputs, pred_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Softwares\\Anaconda3\\envs\\py3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "hidden_size = 512\n",
    "embed_size = 256\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 1\n",
    "encoder_test = Encoder(src.num, embed_size, hidden_size, encoder_n_layers, dropout=0.2)\n",
    "decoder_test = Decoder(embed_size, hidden_size, tgt.num, decoder_n_layers, dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq(\n",
      "  (encoder): Encoder(\n",
      "    (embed): Embedding(46064, 256)\n",
      "    (cell): GRU(256, 512, num_layers=2, dropout=0.2, bidirectional=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embed): Embedding(30499, 256)\n",
      "    (dropout): Dropout(p=0.2, inplace)\n",
      "    (attention): Attention(\n",
      "      (attn): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    )\n",
      "    (cell): GRU(768, 512, dropout=0.2)\n",
      "    (out): Linear(in_features=1024, out_features=30499, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Seq2Seq(encoder_test,decoder_test).cuda()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam(net.parameters(),lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.load_state_dict(torch.load('./saved_model_1.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT:\n",
      "<sos> 这是 我国 几千 年 来 最 深刻 , 最 伟大 的 社会变革 . <eos>\n",
      "REF:\n",
      "<sos> this has been the most profound and the greatest social change that has ever taken place in china over the past several thousand years . <eos> <pad> <pad> <pad>\n",
      "PREDICTION:\n",
      "<sos> this has been the most profound and the greatest that the has ever in china 's social and social years . <eos>\n",
      "BLEU = 0.388644\n",
      "------\n",
      "Epoch 1 Batch Num 200 Train Loss: 1.425621 Test Loss: 1.522334\n",
      "INPUT:\n",
      "<sos> 这次 会议 是 总政治部 委托 北京地区 军队 院校 协作 中心 在 装备 指挥 技术 学院 召开 的 . <eos>\n",
      "REF:\n",
      "<sos> this forum was held in the armament command skills academy by the beijing area military academy coordinating center with the authorization of the general political department . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "PREDICTION:\n",
      "<sos> this forum was held by the armament department of the armament military academy coordinating skills , coordinating the general political department , coordinating the coordinating of the forum . <eos>\n",
      "BLEU = 0.186735\n",
      "------\n",
      "Epoch 1 Batch Num 400 Train Loss: 2.016779 Test Loss: 2.296015\n",
      "INPUT:\n",
      "<sos> 卓玛 一家 人 不论 谁 吃到 什 麽 东西 , 都要 把 面团 亮出来 , 大家 开怀 大笑 , 热闹 非凡 . <eos>\n",
      "REF:\n",
      "<sos> no matter what they found in their dough , zhuoma and her family would quickly show it and then laugh heartily together about it , and it was bustling and exciting , indeed . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "PREDICTION:\n",
      "<sos> it would not like the dough and zhuoma , zhuoma the family family and and it would bustling about it . <eos>\n",
      "BLEU = 0.239319\n",
      "------\n",
      "Epoch 1 Batch Num 600 Train Loss: 2.381748 Test Loss: 2.470115\n",
      "INPUT:\n",
      "<sos> 包括 考虑 雨水 的 利用 , 减少 各种 资源 的 耗用 , 并 注意 提高 防范 和 抵御 自然灾害 的 市场 变化 风险 的 能力 . <eos>\n",
      "REF:\n",
      "<sos> discussing ways to accelerate the construction of small cities and towns , cppcc national committee members pointed out : making use of the market mechanism and exploring various channels and forms of financing are effective means . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "PREDICTION:\n",
      "<sos> various means of various resources and towns of financing , and , and , resources , and , and of various resources of resources , and to the the of of financing and to to the market of various means . <eos>\n",
      "BLEU = 0.158667\n",
      "------\n",
      "Epoch 1 Batch Num 800 Train Loss: 2.795797 Test Loss: 2.853828\n",
      "INPUT:\n",
      "<sos> 这 以后 , 两 革命家 相濡以沫 , 把 深厚 的 情感 融入 对 真理 的 孜孜 追求 和 为 崇高 事业 的 不懈 奋斗 之中 , 成为 党内外 的 楷模 . <eos>\n",
      "REF:\n",
      "<sos> after this , the two revolutionaries were like brush and ink , immersing their deep feelings in the inexhaustible search for truth and in the unflagging struggle of higher undertakings , thus becoming examples for both inside and outside the party . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "PREDICTION:\n",
      "<sos> after becoming the ink , , they were the brush and of the unflagging and the unflagging of the and the the inexhaustible for the the party and the the inexhaustible of becoming a profound struggle for the struggle . <eos>\n",
      "BLEU = 0.451082\n",
      "------\n",
      "Epoch 1 Batch Num 1000 Train Loss: 3.020377 Test Loss: 2.913077\n",
      "INPUT:\n",
      "<sos> 《 暂行 办法 》 改变 了 过去 单一 计划 分配 的 安置 模式 , 采取 了 计划 分配 与 自主 择业 相结合 的 安置 方式 , 将 更加 有利于 转业 干部 人尽其才 , 各得其所 , 为 这 一 人才 资源 的 合理 配置 提供 了 政策 保证 . <eos>\n",
      "REF:\n",
      "<sos> this model will help make full use of the expertise of retired cadres and help these cadres obtain their desired goal . this model also provides a legal guarantee the reasonable distribution of high - quality professional human resources . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "PREDICTION:\n",
      "<sos> this goal provides the the of the the expertise of retired cadres and the expertise of retired this model and provides this guarantee and reasonable retired the distribution of retired quality of resources . <eos>\n",
      "BLEU = 0.148399\n",
      "------\n",
      "Epoch 1 Batch Num 1200 Train Loss: 3.236800 Test Loss: 3.179282\n",
      "epoch 1 finished !\n",
      "INPUT:\n",
      "<sos> 全国 公安 机关 要 坚持 严打 方针 , 依法 严厉 打击 各种 犯罪 活动 , 当前 特别 要 重点 打击 严重 暴力 犯罪 , 有 组织 犯罪 , 毒品 犯罪 和 涉 枪 涉 爆 犯罪 , 坚决 把 犯罪 分子 的 嚣张 气焰 打下去 . <eos>\n",
      "REF:\n",
      "<sos> at the moment , we should especially concentrate on cracking down on crimes of extreme violence , organized crimes , drug - related crimes , gun - related crimes , or explosive - related crimes and should resolutely weed out all criminal elements swollen with arrogance . <eos> <pad> <pad>\n",
      "PREDICTION:\n",
      "<sos> crimes , we should concentrate on crimes elements elements elements - related crimes , crimes , elements , and crimes , elements - related crimes , and explosive - related crimes elements , weed down on crimes elements , crimes , elements explosive crimes . <eos>\n",
      "BLEU = 0.207415\n",
      "------\n",
      "Epoch 2 Batch Num 200 Train Loss: 2.803769 Test Loss: 2.761324\n",
      "INPUT:\n",
      "<sos> 古巴 大使 阿鲁菲 说 , 古巴 和 中国 有 著 长期 友好 的 关系 , 古巴 是 第一 与 中华人民共和国 建立 外交关系 的 拉美 国家 , 古 中 一贯 在 国际 论坛 和 国际 组织 中 保持 合作 , 互相 支持 . <eos>\n",
      "REF:\n",
      "<sos> cuban ambassador alberto rodriguez arufe said that cuba and china have had a long friendship and cuba was the first latin american country to establish diplomatic relations with the prc . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "PREDICTION:\n",
      "<sos> the cuban ambassador alberto rodriguez arufe of china and cuba have been cuba with the cuban latin american relations have been able to latin america , and the have been able to maintain diplomatic relations with the international of the first latin . <eos>\n",
      "BLEU = 0.172761\n",
      "------\n",
      "Epoch 2 Batch Num 400 Train Loss: 2.714005 Test Loss: 2.451576\n",
      "INPUT:\n",
      "<sos> 坚持 党 管 干部 原则 , 改进 管理 方法 , 对 不同 类型 事业单位 的 领导 人员 , 按照 干部 管理 权限 和 一定 程序 , 可 实行 直接 聘任 , 招标 聘任 , 推选 聘任 , 委任 等 多种 任用 形式 . <eos>\n",
      "REF:\n",
      "<sos> adhere to the principle of having the party manage cadres ; improve management methods ; it is proper to use various forms of hiring , including direct hiring , bid - invitation hiring , recommendation hiring , and appointment . <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "PREDICTION:\n",
      "<sos> in the course of the party , hiring , hiring , hiring , hiring , hiring , hiring , hiring hiring , hiring management , hiring , hiring management , and hiring , hiring management , and hiring . <eos>\n",
      "BLEU = 0.406678\n",
      "------\n",
      "Epoch 2 Batch Num 600 Train Loss: 2.750260 Test Loss: 2.939461\n",
      "INPUT:\n",
      "<sos> 在 抢救 过程 中 , 自焚 者 称 他 是 \" 学 ' 法 轮 大法 ' 的 , 因为 心性 太 差 , 有 ' 执著 ' , 要 去掉 那些 东西 , 所以 要 把 它 烧掉 \" . <eos>\n",
      "REF:\n",
      "<sos> in the course of emergency treatment , the self - immolator said he was \" learning ' falun dafa , ' but , realizing that he was too bad tempered and had ' obsessions , ' decided he should have these undesirable things burned away . \" <eos> <pad> <pad>\n",
      "PREDICTION:\n",
      "<sos> in the last falun ' falun ' falun dafa ' falun dafa he was tempered : \" falun dafa ' falun dafa , he he he he tempered their falun ' ' falun dafa . he was tempered . \" <eos>\n",
      "BLEU = 0.103272\n",
      "------\n",
      "Epoch 2 Batch Num 800 Train Loss: 2.709144 Test Loss: 2.765850\n",
      "INPUT:\n",
      "<sos> 高尔 与 工会 的 关系 匪 浅 , 当初 他 曾 为了 讨好 工会 领袖 , 在 讨论 给予 中共 \" 永久 正常 贸易 关系 \" 时 建议 加上 附带 条件 , 后来 在 白宫 的 强大 压力 下 让步 . <eos>\n",
      "REF:\n",
      "<sos> gore has maintained close ties with the trade unions . to curry favor with trade union leaders , gore proposed granting china \" permanent normal trade relations \" status with strings attached but later made concessions when faced with strong pressure from the white house . <eos>\n",
      "PREDICTION:\n",
      "<sos> the white house has attached great pressure to the trade with the trade union , gore , with gore , trade relations with trade union , \" trade relations with trade union , \" trade relations with trade union , and trade relations with strings to trade relations with\n",
      "BLEU = 0.272453\n",
      "------\n",
      "Epoch 2 Batch Num 1000 Train Loss: 2.725503 Test Loss: 2.677639\n",
      "INPUT:\n",
      "<sos> 一方面 广泛 发动 群众 , 形成 宣传 声势 , 另一方面 继续 开展 有 针对性 的 打击 和 全面 收缴 工作 , 在 严格 检查 , 规范 管理 , 严密 控制 的 基础 上 , 进一步 研究 和 落实 了 治 爆 缉 枪 等 治安 管理 的 长效 机制 . <eos>\n",
      "REF:\n",
      "<sos> in addition , effective mechanisms of security administration to stamp out explosives , seize guns , and deal with other problems have been further researched and carried out on a foundation of strict examination , strict regulation , strict management , and rigorous control . <eos> <pad> <pad> <pad> <pad>\n",
      "PREDICTION:\n",
      "<sos> on the other hand , the , and , , , , , , , , , , comprehensively on the the , , , , comprehensively put on the issues , strict management , , comprehensively strict management , strict management , and , strict management , and\n",
      "BLEU = 0.099036\n",
      "------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch Num 1200 Train Loss: 2.732009 Test Loss: 2.522732\n",
      "epoch 2 finished !\n",
      "INPUT:\n",
      "<sos> 据介绍 , 这 两 大 频道 的 开设 , 体现 了 中央 电视台 对 频道 整体 布局 的 日臻 全面 , 形成 了 一个 以 一套 节目 为 龙头 , 以 综合 频道 , 专业 频道 门类 日趋 齐全 为 特征 的 网络 体系 . <eos>\n",
      "REF:\n",
      "<sos> it is learned that the launch of the two channels shows cctv 's increasingly comprehensive overall programming . it has established a network with program one as its principal channel and included increasingly complete categories of general and special channels in its programming . <eos> <pad> <pad> <pad> <pad>\n",
      "PREDICTION:\n",
      "<sos> it has learned that the establishment of the central system has increasingly cctv , it has increasingly increasingly a comprehensive channel of the overall channels of its overall channels and its overall channels of its overall channels and its overall channels . <eos>\n",
      "BLEU = 0.181869\n",
      "------\n",
      "Epoch 3 Batch Num 200 Train Loss: 2.642128 Test Loss: 2.458873\n"
     ]
    }
   ],
   "source": [
    "grad_clip = 5\n",
    "num_batch = len(pairs) // batch_size\n",
    "print_every_batches = 200\n",
    "save_every_batches = 200\n",
    "valid_every_epochs = print_every_batches\n",
    "pairs.sort(key=lambda x: len(x[0].split()))\n",
    "\n",
    "for epoch in range(1,50000):\n",
    "    total_loss = 0\n",
    "    tmp_loss = 0\n",
    "    for batch_idx in range(num_batch):\n",
    "        input_batches, input_lengths,\\\n",
    "            target_batches, target_lengths = random_batch(src,tgt,pairs,batch_size,batch_idx)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        output = net(input_batches, input_lengths, target_batches, target_lengths)\n",
    "\n",
    "        #loss = masked_cross_entropy.compute_loss(\n",
    "        #    output.transpose(0, 1).contiguous(),\n",
    "        #    target_batches.transpose(0, 1).contiguous(),\n",
    "        #    target_lengths\n",
    "        #)\n",
    "        loss = F.nll_loss(output[1:].view(-1,tgt.num),\n",
    "                          target_batches[1:].contiguous().view(-1),\n",
    "                          ignore_index=PAD_idx)\n",
    "        \n",
    "        tmp_loss += loss.item()\n",
    "        if (batch_idx+1) % save_every_batches == 0:\n",
    "            torch.save(net.state_dict(), './saved.pt')\n",
    "        clip_grad_norm_(net.parameters(), grad_clip)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        if (batch_idx+1) % print_every_batches == 0:\n",
    "            opt.zero_grad()\n",
    "            input_batches, input_lengths,\\\n",
    "                target_batches, target_lengths = random_batch(src,tgt,pairs,batch_size,batch_idx)\n",
    "            for test_idx in range(1):\n",
    "                _, pred = net.inference(input_batches[:,test_idx].reshape(input_lengths[0].item(),1),input_lengths[0].reshape(1))\n",
    "                inp = ' '.join([src.idx2w[t] for t in input_batches[:,test_idx].cpu().numpy()])\n",
    "                mt = ' '.join([tgt.idx2w[t] for t in pred if t!= PAD_idx])\n",
    "                ref = ' '.join([tgt.idx2w[t] for t in target_batches[:,test_idx].cpu().numpy()])\n",
    "                print('INPUT:\\n' + inp)\n",
    "                print('REF:\\n' + ref)\n",
    "                print('PREDICTION:\\n' + mt)\n",
    "                print('BLEU = %f' % bleu([mt],[[ref]],4))\n",
    "                print(\"------\")\n",
    "            output = net(input_batches, input_lengths, target_batches, target_lengths)\n",
    "\n",
    "            #loss = masked_cross_entropy.compute_loss(\n",
    "            #    output.transpose(0, 1).contiguous(),\n",
    "            #    target_batches.transpose(0, 1).contiguous(),\n",
    "            #    target_lengths\n",
    "            #)\n",
    "            loss = F.nll_loss(output[1:].view(-1,tgt.num),\n",
    "                              target_batches[1:].contiguous().view(-1),\n",
    "                              ignore_index=PAD_idx)\n",
    "            print(\"Epoch %d Batch Num %d Train Loss: %f Test Loss: %f\"%(epoch, batch_idx+1, tmp_loss/print_every_batches, loss.item()))\n",
    "            tmp_loss = 0\n",
    "\n",
    "    print('epoch %d finished !'%(epoch))\n",
    "    random.shuffle(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Inference result of train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs.sort(key=lambda x: len(x[0].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT:\n",
      "<sos> 江苏 宜兴市 . <eos>\n",
      "REF:\n",
      "<sos> yixing city , jiangsu . <eos>\n",
      "PREDICTION:\n",
      "<sos> yixing city , jiangsu was <eos>\n",
      "BLEU = 0.643459\n",
      "------\n",
      "INPUT:\n",
      "<sos> 总理 朱鎔基 二零零一年八月二日 <eos>\n",
      "REF:\n",
      "<sos> prime minister zhu rongji 2 august 2001 <eos>\n",
      "PREDICTION:\n",
      "<sos> prime minister zhu rongji 2 august 2001 <eos>\n",
      "BLEU = 1.000000\n",
      "------\n",
      "INPUT:\n",
      "<sos> 再说 科索沃 . <eos>\n",
      "REF:\n",
      "<sos> or take kosovo . <eos>\n",
      "PREDICTION:\n",
      "<sos> take take kosovo . <eos>\n",
      "BLEU = 0.537285\n",
      "------\n",
      "INPUT:\n",
      "<sos> 1995年 离休 . <eos>\n",
      "REF:\n",
      "<sos> he retired in 1995 . <eos>\n",
      "PREDICTION:\n",
      "<sos> in 1995 and the . <eos>\n",
      "BLEU = 0.698534\n",
      "------\n",
      "INPUT:\n",
      "<sos> 谢谢 各位 ! <eos>\n",
      "REF:\n",
      "<sos> thank you ! <eos>\n",
      "PREDICTION:\n",
      "<sos> thank you ! <eos>\n",
      "BLEU = 1.000000\n",
      "------\n",
      "INPUT:\n",
      "<sos> 杜青林 说 . <eos>\n",
      "REF:\n",
      "<sos> du qinglin said . <eos>\n",
      "PREDICTION:\n",
      "<sos> du qinglin said . <eos>\n",
      "BLEU = 1.000000\n",
      "------\n",
      "INPUT:\n",
      "<sos> ( 王永霞 ) <eos>\n",
      "REF:\n",
      "<sos> ( by wang yongxia ) <eos>\n",
      "PREDICTION:\n",
      "<sos> ( by wang yongxia ) <eos>\n",
      "BLEU = 1.000000\n",
      "------\n",
      "INPUT:\n",
      "<sos> 朱鎔基 问 . <eos>\n",
      "REF:\n",
      "<sos> zhu rongji asked . <eos>\n",
      "PREDICTION:\n",
      "<sos> zhu rongji asked . <eos>\n",
      "BLEU = 1.000000\n",
      "------\n",
      "INPUT:\n",
      "<sos> 袁木 否认 参与 <eos>\n",
      "REF:\n",
      "<sos> this is the best example of peaceful evolution . <eos>\n",
      "PREDICTION:\n",
      "<sos> this is the best example of peaceful evolution . <eos>\n",
      "BLEU = 1.000000\n",
      "------\n",
      "INPUT:\n",
      "<sos> 军政 融合 . <eos>\n",
      "REF:\n",
      "<sos> integrate military and political matters . <eos>\n",
      "PREDICTION:\n",
      "<sos> integrate and military affairs . <eos>\n",
      "BLEU = 0.633783\n",
      "------\n",
      "INPUT:\n",
      "<sos> 拜登 来了 . <eos>\n",
      "REF:\n",
      "<sos> then came joseph biden . <eos>\n",
      "PREDICTION:\n",
      "<sos> then came joseph biden . <eos>\n",
      "BLEU = 1.000000\n",
      "------\n",
      "INPUT:\n",
      "<sos> 居心安 在 ? <eos>\n",
      "REF:\n",
      "<sos> what are its motives ? <eos>\n",
      "PREDICTION:\n",
      "<sos> what are its motives ? <eos>\n",
      "BLEU = 1.000000\n",
      "------\n",
      "INPUT:\n",
      "<sos> 向钱看 腐化 解放军 <eos>\n",
      "REF:\n",
      "<sos> the pla is being corrupted by mammonism . <eos>\n",
      "PREDICTION:\n",
      "<sos> the pla is being corrupted by mammonism . <eos>\n",
      "BLEU = 1.000000\n",
      "------\n",
      "INPUT:\n",
      "<sos> 依仗 高科技 优势 <eos>\n",
      "REF:\n",
      "<sos> rely on high tech superiority . <eos>\n",
      "PREDICTION:\n",
      "<sos> rely on high tech superiority . <eos>\n",
      "BLEU = 1.000000\n",
      "------\n",
      "INPUT:\n",
      "<sos> ( 易才 ) <eos>\n",
      "REF:\n",
      "<sos> 4 ) the number of investors will double . <eos>\n",
      "PREDICTION:\n",
      "<sos> 4 ) the number of investors will double the . <eos>\n",
      "BLEU = 0.769161\n",
      "------\n",
      "INPUT:\n",
      "<sos> 网络 一体 . <eos>\n",
      "REF:\n",
      "<sos> integration of networks . <eos>\n",
      "PREDICTION:\n",
      "<sos> integration networks networks . <eos>\n",
      "BLEU = 0.594604\n",
      "------\n",
      "INPUT:\n",
      "<sos> 占据 市场 . <eos>\n",
      "REF:\n",
      "<sos> occupying markets . <eos>\n",
      "PREDICTION:\n",
      "<sos> occupying markets . <eos>\n",
      "BLEU = 1.000000\n",
      "------\n",
      "INPUT:\n",
      "<sos> 同意 ! \" <eos>\n",
      "REF:\n",
      "<sos> yes we do ! \" <eos>\n",
      "PREDICTION:\n",
      "<sos> yes we ! ! \" <eos>\n",
      "BLEU = 0.691442\n",
      "------\n",
      "INPUT:\n",
      "<sos> 切忌 形式主义 . <eos>\n",
      "REF:\n",
      "<sos> formalism should be avoided by all means . <eos>\n",
      "PREDICTION:\n",
      "<sos> formalism should be avoided by all means . <eos>\n",
      "BLEU = 1.000000\n",
      "------\n",
      "INPUT:\n",
      "<sos> 能否 水落石出 ? <eos>\n",
      "REF:\n",
      "<sos> will the truth come out ? <eos>\n",
      "PREDICTION:\n",
      "<sos> will the truth come out ? <eos>\n",
      "BLEU = 1.000000\n",
      "------\n",
      "INPUT:\n",
      "<sos> 会议 上 发言 <eos>\n",
      "REF:\n",
      "<sos> following is the full text of the speech . <eos>\n",
      "PREDICTION:\n",
      "<sos> following the full text of the speech . <eos>\n",
      "BLEU = 0.751650\n",
      "------\n",
      "INPUT:\n",
      "<sos> 走 自己的 路 <eos>\n",
      "REF:\n",
      "<sos> walk in one 's own route . <eos>\n",
      "PREDICTION:\n",
      "<sos> walk in one own own route . <eos>\n",
      "BLEU = 0.596949\n",
      "------\n",
      "INPUT:\n",
      "<sos> 谢谢 各位 . <eos>\n",
      "REF:\n",
      "<sos> thank you . <eos>\n",
      "PREDICTION:\n",
      "<sos> thank you . <eos>\n",
      "BLEU = 1.000000\n",
      "------\n",
      "INPUT:\n",
      "<sos> 重点 发展 . <eos>\n",
      "REF:\n",
      "<sos> we should develop in major aspects . <eos>\n",
      "PREDICTION:\n",
      "<sos> we should develop the major aspects . <eos>\n",
      "BLEU = 0.596949\n",
      "------\n",
      "INPUT:\n",
      "<sos> 结局 如何 ? <eos>\n",
      "REF:\n",
      "<sos> what will be the result ? <eos>\n",
      "PREDICTION:\n",
      "<sos> how will be be the result ? <eos>\n",
      "BLEU = 0.513345\n",
      "------\n",
      "INPUT:\n",
      "<sos> 答案 昭然若揭 . <eos>\n",
      "REF:\n",
      "<sos> here , the answer is quite clear . <eos>\n",
      "PREDICTION:\n",
      "<sos> here , the is is quite clear . <eos>\n",
      "BLEU = 0.658037\n",
      "------\n",
      "INPUT:\n",
      "<sos> 此 其一 . <eos>\n",
      "REF:\n",
      "<sos> that is the first point . <eos>\n",
      "PREDICTION:\n",
      "<sos> that is the first point . <eos>\n",
      "BLEU = 1.000000\n",
      "------\n",
      "INPUT:\n",
      "<sos> 何谓 揭露 ? <eos>\n",
      "REF:\n",
      "<sos> what does \" expose \" mean then ? <eos>\n",
      "PREDICTION:\n",
      "<sos> what does the \" expose \" mean then ? <eos>\n",
      "BLEU = 0.701688\n",
      "------\n",
      "INPUT:\n",
      "<sos> theusgovernmenthasdeliveredaletterofapologytothechinesesideinconnectionwiththeincidentinwhichausmilitaryreconnaissanceplanerammedanddestroyedachineseaircraft.outofhumanitarianconsiderations , thechinesegovernmenthasdecidedtolettheuscrewmembersleavethecountry. <eos>\n",
      "REF:\n",
      "<sos> this incident has yet to be completely settled . <eos>\n",
      "PREDICTION:\n",
      "<sos> this incident has yet to be completely settled . <eos>\n",
      "BLEU = 1.000000\n",
      "------\n",
      "INPUT:\n",
      "<sos> 值得 深思 . <eos>\n",
      "REF:\n",
      "<sos> it deserves careful pondering . <eos>\n",
      "PREDICTION:\n",
      "<sos> it deserves careful pondering . <eos>\n",
      "BLEU = 1.000000\n",
      "------\n",
      "INPUT:\n",
      "<sos> 责任编辑 : 田颖 <eos>\n",
      "REF:\n",
      "<sos> responsible editor : tian ying <eos>\n",
      "PREDICTION:\n",
      "<sos> responsible : tian ying . <eos>\n",
      "BLEU = 0.541082\n",
      "------\n",
      "INPUT:\n",
      "<sos> 会议 的 背景 <eos>\n",
      "REF:\n",
      "<sos> historically , this was rather significant . <eos>\n",
      "PREDICTION:\n",
      "<sos> historically , this was rather significant . <eos>\n",
      "BLEU = 1.000000\n",
      "------\n",
      "INPUT:\n",
      "<sos> 这是 事实 . <eos>\n",
      "REF:\n",
      "<sos> this is a fact . <eos>\n",
      "PREDICTION:\n",
      "<sos> this is a fact . <eos>\n",
      "BLEU = 1.000000\n",
      "------\n",
      "INPUT:\n",
      "<sos> 中美 关系 何去何从 <eos>\n",
      "REF:\n",
      "<sos> where will sino - us relations go ? <eos>\n",
      "PREDICTION:\n",
      "<sos> where will sino - us relations go go down . <eos>\n",
      "BLEU = 0.631555\n",
      "------\n",
      "INPUT:\n",
      "<sos> 人民 网 2001年4月22日 <eos>\n",
      "REF:\n",
      "<sos> the japanese politicians are victimizing them . <eos>\n",
      "PREDICTION:\n",
      "<sos> the japanese politicians are victimizing them . <eos>\n",
      "BLEU = 1.000000\n",
      "------\n",
      "INPUT:\n",
      "<sos> ( 2000年11月9日 ) <eos>\n",
      "REF:\n",
      "<sos> november 9 , 2000 <eos>\n",
      "PREDICTION:\n",
      "<sos> november 9 . <eos>\n",
      "BLEU = 0.494739\n",
      "------\n",
      "INPUT:\n",
      "<sos> 海风 拂面 . <eos>\n",
      "REF:\n",
      "<sos> the sea breeze was blowing . <eos>\n",
      "PREDICTION:\n",
      "<sos> the sea breeze was blowing . <eos>\n",
      "BLEU = 1.000000\n",
      "------\n",
      "INPUT:\n",
      "<sos> 第三 罚 则 <eos>\n",
      "REF:\n",
      "<sos> chapter three . <eos>\n",
      "PREDICTION:\n",
      "<sos> chapter three . <eos>\n",
      "BLEU = 1.000000\n",
      "------\n",
      "INPUT:\n",
      "<sos> 演练 镜头 . <eos>\n",
      "REF:\n",
      "<sos> the scene of the drill . <eos>\n",
      "PREDICTION:\n",
      "<sos> the scene of the drill . <eos>\n",
      "BLEU = 1.000000\n",
      "------\n",
      "INPUT:\n",
      "<sos> 史可鉴 今 . <eos>\n",
      "REF:\n",
      "<sos> the past can be used for reference today . <eos>\n",
      "PREDICTION:\n",
      "<sos> the past can be used for reference today . <eos>\n",
      "BLEU = 1.000000\n",
      "------\n",
      "INPUT:\n",
      "<sos> 理由 很多 . <eos>\n",
      "REF:\n",
      "<sos> there are many reasons . <eos>\n",
      "PREDICTION:\n",
      "<sos> there are many reasons . <eos>\n",
      "BLEU = 1.000000\n",
      "------\n",
      "INPUT:\n",
      "<sos> 是否 先进 ? <eos>\n",
      "REF:\n",
      "<sos> are they advanced weapons ? <eos>\n",
      "PREDICTION:\n",
      "<sos> are they advanced weapons ? <eos>\n",
      "BLEU = 1.000000\n",
      "------\n",
      "INPUT:\n",
      "<sos> 凡事 开头难 . <eos>\n",
      "REF:\n",
      "<sos> every endeavor is difficult at the beginning . <eos>\n",
      "PREDICTION:\n",
      "<sos> every endeavor is the first of the . <eos>\n",
      "BLEU = 0.324668\n",
      "------\n",
      "INPUT:\n",
      "<sos> 伯杰 说 . <eos>\n",
      "REF:\n",
      "<sos> berger said . <eos>\n",
      "PREDICTION:\n",
      "<sos> berger said . <eos>\n",
      "BLEU = 1.000000\n",
      "------\n",
      "INPUT:\n",
      "<sos> 拖垮 对手 . <eos>\n",
      "REF:\n",
      "<sos> wearing down the opponents . <eos>\n",
      "PREDICTION:\n",
      "<sos> wearing down on opponents . <eos>\n",
      "BLEU = 0.691442\n",
      "------\n",
      "INPUT:\n",
      "<sos> 形势逼人 啊 ! <eos>\n",
      "REF:\n",
      "<sos> the situation is pressing ! <eos>\n",
      "PREDICTION:\n",
      "<sos> the situation is pressing ! <eos>\n",
      "BLEU = 1.000000\n",
      "------\n",
      "INPUT:\n",
      "<sos> 拉萨 ! \" <eos>\n",
      "REF:\n",
      "<sos> this always confuses new comers . <eos>\n",
      "PREDICTION:\n",
      "<sos> always confuses new comers . <eos>\n",
      "BLEU = 0.728955\n",
      "------\n",
      "INPUT:\n",
      "<sos> 难道 不是 ? <eos>\n",
      "REF:\n",
      "<sos> is that not so ? <eos>\n",
      "PREDICTION:\n",
      "<sos> is not so that ? <eos>\n",
      "BLEU = 0.840896\n",
      "------\n",
      "INPUT:\n",
      "<sos> 空间 一体 . <eos>\n",
      "REF:\n",
      "<sos> integration of space . <eos>\n",
      "PREDICTION:\n",
      "<sos> integration of space space . <eos>\n",
      "BLEU = 0.572125\n",
      "------\n",
      "INPUT:\n",
      "<sos> 国家主席 : 胡锦涛 . <eos>\n",
      "REF:\n",
      "<sos> president : hu jintao . <eos>\n",
      "PREDICTION:\n",
      "<sos> president : hu jintao . <eos>\n",
      "BLEU = 1.000000\n",
      "------\n",
      "Average BLEU = 0.8442469412456157\n"
     ]
    }
   ],
   "source": [
    "from infer_eval import bleu\n",
    "\n",
    "test_range = 50\n",
    "ave_bleu = 0\n",
    "for test_idx in range(test_range):\n",
    "    input_batches, input_lengths,\\\n",
    "        target_batches, target_lengths = random_batch(src,tgt,pairs,1,test_idx)\n",
    "    _, pred = net.inference(input_batches[:,0].reshape(input_lengths[0].item(),1),input_lengths[0].reshape(1))\n",
    "    inp = ' '.join([src.idx2w[t] for t in input_batches[:,0].cpu().numpy()])\n",
    "    mt = ' '.join([tgt.idx2w[t] for t in pred if t!= PAD_idx])\n",
    "    ref = ' '.join([tgt.idx2w[t] for t in target_batches[:,0].cpu().numpy() if t != PAD_idx])\n",
    "    print('INPUT:\\n' + inp)\n",
    "    print('REF:\\n' + ref)\n",
    "    print('PREDICTION:\\n' + mt)\n",
    "    tmp_score = bleu([mt],[[ref]],4)\n",
    "    ave_bleu += tmp_score\n",
    "    print('BLEU = %f' % tmp_score)\n",
    "    print(\"------\")\n",
    "print('Average BLEU = '+str(ave_bleu/test_range))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check inference result of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 10000 sentence pairs\n",
      "Trimmed to 8572 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "chinese 17294\n",
      "english 13049\n",
      "chinese:\n",
      "Total words 17294\n",
      "After Trimming 17290\n",
      "Keep Ratio % 99.9768705909564\n",
      "english:\n",
      "Total words 13049\n",
      "After Trimming 13045\n",
      "Keep Ratio % 99.96934631006208\n",
      "['曾宪梓 说 : \" 我 的 立场 没有 改变 , 我 的 立场 是 坚决 拥护 中央政府 的 政策 , 这点 很 明确 .', 'tsang said : \" i have not changed my position , which is to resolutely the central government policy .']\n"
     ]
    }
   ],
   "source": [
    "test_src, test_tgt, test_pairs = prepareData('data/test.txt', 'english', 'chinese')\n",
    "test_src.trim()\n",
    "test_tgt.trim()\n",
    "print(random.choice(test_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_src.w2idx, test_src.idx2w, test_src.num = src.w2idx, src.idx2w, src.num\n",
    "test_tgt.w2idx, test_tgt.idx2w, test_tgt.num = tgt.w2idx, tgt.idx2w, tgt.num\n",
    "test_pairs.sort(key=lambda x: len(x[0].split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT:\n",
      "<sos> 时光 <unk> . <eos>\n",
      "REF:\n",
      "<sos> time really flies like a shuttle . <eos>\n",
      "PREDICTION:\n",
      "<sos> as a total of whole . <eos>\n",
      "BLEU = 0.456227\n",
      "------\n",
      "INPUT:\n",
      "<sos> 分担 经费 . <eos>\n",
      "REF:\n",
      "<sos> sharing the expense . <eos>\n",
      "PREDICTION:\n",
      "<sos> there are for funds . <eos>\n",
      "BLEU = 0.516973\n",
      "------\n",
      "INPUT:\n",
      "<sos> 中国 ! \" <eos>\n",
      "REF:\n",
      "<sos> china ! \" <eos>\n",
      "PREDICTION:\n",
      "<sos> \" china ! ! \" <eos>\n",
      "BLEU = 0.516973\n",
      "------\n",
      "INPUT:\n",
      "<sos> 提高 素质 迫在眉睫 <eos>\n",
      "REF:\n",
      "<sos> this is the only way to succeed . <eos>\n",
      "PREDICTION:\n",
      "<sos> raising the quality of raising their urgent . <eos>\n",
      "BLEU = 0.459150\n",
      "------\n",
      "INPUT:\n",
      "<sos> 权责 一体 . <eos>\n",
      "REF:\n",
      "<sos> integration of right and responsibility . <eos>\n",
      "PREDICTION:\n",
      "<sos> there are no . <eos>\n",
      "BLEU = 0.402935\n",
      "------\n",
      "INPUT:\n",
      "<sos> 显然 不是 . <eos>\n",
      "REF:\n",
      "<sos> obvious not . <eos>\n",
      "PREDICTION:\n",
      "<sos> obviously , not a . <eos>\n",
      "BLEU = 0.555524\n",
      "------\n",
      "INPUT:\n",
      "<sos> 你们 一定 能行 ! <eos>\n",
      "REF:\n",
      "<sos> you surely can do it ! <eos>\n",
      "PREDICTION:\n",
      "<sos> you you you you ! <eos>\n",
      "BLEU = 0.572688\n",
      "------\n",
      "INPUT:\n",
      "<sos> 王兆国 主持 座谈会 . <eos>\n",
      "REF:\n",
      "<sos> the symposium was presided over by wang zhaoguo . <eos>\n",
      "PREDICTION:\n",
      "<sos> wang zhaoguo 's wang zhaoguo chaired . <eos>\n",
      "BLEU = 0.488829\n",
      "------\n",
      "INPUT:\n",
      "<sos> \" 继续 上学 . <eos>\n",
      "REF:\n",
      "<sos> \" i want to continue going to school . <eos>\n",
      "PREDICTION:\n",
      "<sos> continue to continue the . . <eos>\n",
      "BLEU = 0.446779\n",
      "------\n",
      "INPUT:\n",
      "<sos> 伙 <unk> 长 受贿 <eos>\n",
      "REF:\n",
      "<sos> but , she refused to give specific details . <eos>\n",
      "PREDICTION:\n",
      "<sos> the group of a group for the comments will be . <eos>\n",
      "BLEU = 0.372391\n",
      "------\n",
      "INPUT:\n",
      "<sos> 其 动机 何在 ? <eos>\n",
      "REF:\n",
      "<sos> what is their motive ? <eos>\n",
      "PREDICTION:\n",
      "<sos> what is the people 's things ? <eos>\n",
      "BLEU = 0.415351\n",
      "------\n",
      "INPUT:\n",
      "<sos> 果真 如此 吗 ? <eos>\n",
      "REF:\n",
      "<sos> is that really so ? <eos>\n",
      "PREDICTION:\n",
      "<sos> is it really really ? <eos>\n",
      "BLEU = 0.698534\n",
      "------\n",
      "INPUT:\n",
      "<sos> 王恩茂 同志 永垂不朽 ! <eos>\n",
      "REF:\n",
      "<sos> eternal glory to comrade wang enmao ! <eos>\n",
      "PREDICTION:\n",
      "<sos> comrade comrades who begin the ! ! <eos>\n",
      "BLEU = 0.485492\n",
      "------\n",
      "INPUT:\n",
      "<sos> 内容 实在 发人深省 . <eos>\n",
      "REF:\n",
      "<sos> the content of this book is very thought - provoking . <eos>\n",
      "PREDICTION:\n",
      "<sos> the actually is there there is a content . <eos>\n",
      "BLEU = 0.479164\n",
      "------\n",
      "INPUT:\n",
      "<sos> 发展 者 无畏 . <eos>\n",
      "REF:\n",
      "<sos> developers are dauntless . <eos>\n",
      "PREDICTION:\n",
      "<sos> we are developed in those with those with those . <eos>\n",
      "BLEU = 0.417226\n",
      "------\n",
      "INPUT:\n",
      "<sos> 还是 两国 关系 ? <eos>\n",
      "REF:\n",
      "<sos> or is it relations between two countries ? <eos>\n",
      "PREDICTION:\n",
      "<sos> are two relationship that or two relations ? <eos>\n",
      "BLEU = 0.508133\n",
      "------\n",
      "INPUT:\n",
      "<sos> 南北 关系 受 牵制 <eos>\n",
      "REF:\n",
      "<sos> relationship between south and north koreas is restrained . <eos>\n",
      "PREDICTION:\n",
      "<sos> north - south relations have north relations has been north . <eos>\n",
      "BLEU = 0.423118\n",
      "------\n",
      "INPUT:\n",
      "<sos> 能 维持 多久 ? <eos>\n",
      "REF:\n",
      "<sos> how long can it be maintained ? <eos>\n",
      "PREDICTION:\n",
      "<sos> how can they able to maintain ? <eos>\n",
      "BLEU = 0.610474\n",
      "------\n",
      "INPUT:\n",
      "<sos> 这是 彼此的 默契 . <eos>\n",
      "REF:\n",
      "<sos> this is a tacit agreement between the two . <eos>\n",
      "PREDICTION:\n",
      "<sos> this is an shocking . <eos>\n",
      "BLEU = 0.291944\n",
      "------\n",
      "INPUT:\n",
      "<sos> 改革开放 20 年 . <eos>\n",
      "REF:\n",
      "<sos> twenty years of reform and opening up . <eos>\n",
      "PREDICTION:\n",
      "<sos> the two sides had undergone 20 launch 20 years . <eos>\n",
      "BLEU = 0.417226\n",
      "------\n",
      "INPUT:\n",
      "<sos> \" 点火 ! \" <eos>\n",
      "REF:\n",
      "<sos> \" ignition ! \" <eos>\n",
      "PREDICTION:\n",
      "<sos> \" ignition ! \" <eos>\n",
      "BLEU = 1.000000\n",
      "------\n",
      "INPUT:\n",
      "<sos> 合 则 两利 . <eos>\n",
      "REF:\n",
      "<sos> cooperation will benefit both sides . <eos>\n",
      "PREDICTION:\n",
      "<sos> this is is an effective energy . <eos>\n",
      "BLEU = 0.451801\n",
      "------\n",
      "INPUT:\n",
      "<sos> 答案 就是 科学 . <eos>\n",
      "REF:\n",
      "<sos> the answer is science . <eos>\n",
      "PREDICTION:\n",
      "<sos> a scientific answer is to be . <eos>\n",
      "BLEU = 0.610474\n",
      "------\n",
      "INPUT:\n",
      "<sos> 美方 需要 道歉 . <eos>\n",
      "REF:\n",
      "<sos> the united states needs to apologize . <eos>\n",
      "PREDICTION:\n",
      "<sos> the united side needs the us side . <eos>\n",
      "BLEU = 0.397635\n",
      "------\n",
      "INPUT:\n",
      "<sos> <unk> ( <unk> ) <eos>\n",
      "REF:\n",
      "<sos> all these regulations are relevant to the lc business . <eos>\n",
      "PREDICTION:\n",
      "<sos> ( by jin ying ) ) <eos>\n",
      "BLEU = 0.428882\n",
      "------\n",
      "INPUT:\n",
      "<sos> 很 可怕 ! \" <eos>\n",
      "REF:\n",
      "<sos> it is really horrible ! \" <eos>\n",
      "PREDICTION:\n",
      "<sos> this very ! ! \" <eos>\n",
      "BLEU = 0.382980\n",
      "------\n",
      "INPUT:\n",
      "<sos> 开放 式 育才 . <eos>\n",
      "REF:\n",
      "<sos> an open mode of teaching talent . <eos>\n",
      "PREDICTION:\n",
      "<sos> the open of the open . <eos>\n",
      "BLEU = 0.482402\n",
      "------\n",
      "INPUT:\n",
      "<sos> 消费 价格 小幅 攀升 . <eos>\n",
      "REF:\n",
      "<sos> consumer prices climbed by a small margin . <eos>\n",
      "PREDICTION:\n",
      "<sos> the price consumer prices is be . <eos>\n",
      "BLEU = 0.546276\n",
      "------\n",
      "INPUT:\n",
      "<sos> 记者 向 周永康 问道 . <eos>\n",
      "REF:\n",
      "<sos> zhou yongkang was asked by a reporter . <eos>\n",
      "PREDICTION:\n",
      "<sos> they are reporters to reporters on this . <eos>\n",
      "BLEU = 0.427287\n",
      "------\n",
      "INPUT:\n",
      "<sos> 这种 局面 需要 改变 . <eos>\n",
      "REF:\n",
      "<sos> such state of affairs should be changed . <eos>\n",
      "PREDICTION:\n",
      "<sos> such change situation needs . <eos>\n",
      "BLEU = 0.430362\n",
      "------\n",
      "INPUT:\n",
      "<sos> 第六 , 强化 企业管理 . <eos>\n",
      "REF:\n",
      "<sos> sixth , it is necessary to strengthen enterprise management . <eos>\n",
      "PREDICTION:\n",
      "<sos> sixth , it is a sixth management . <eos>\n",
      "BLEU = 0.430146\n",
      "------\n",
      "INPUT:\n",
      "<sos> 这是 举世 公认 的 . <eos>\n",
      "REF:\n",
      "<sos> this is acknowledged by the whole world . <eos>\n",
      "PREDICTION:\n",
      "<sos> this is a acknowledged acknowledged acknowledged . <eos>\n",
      "BLEU = 0.389005\n",
      "------\n",
      "INPUT:\n",
      "<sos> 十六 , 删去 <unk> . <eos>\n",
      "REF:\n",
      "<sos> article 87 is sloughed off . <eos>\n",
      "PREDICTION:\n",
      "<sos> so they , they can be carried out of 16 . <eos>\n",
      "BLEU = 0.372391\n",
      "------\n",
      "INPUT:\n",
      "<sos> 李鹏 对此 表示 赞同 . <eos>\n",
      "REF:\n",
      "<sos> li peng agreed with him . <eos>\n",
      "PREDICTION:\n",
      "<sos> li peng agreed with this with li . <eos>\n",
      "BLEU = 0.451801\n",
      "------\n",
      "INPUT:\n",
      "<sos> 人民军队 来自 于 人民 . <eos>\n",
      "REF:\n",
      "<sos> the people 's army came from the people . <eos>\n",
      "PREDICTION:\n",
      "<sos> people are people from the whole . <eos>\n",
      "BLEU = 0.531727\n",
      "------\n",
      "INPUT:\n",
      "<sos> 听听 赖斯 的话 吧 . <eos>\n",
      "REF:\n",
      "<sos> let us hear what rice has said . <eos>\n",
      "PREDICTION:\n",
      "<sos> his family exactly rice . <eos>\n",
      "BLEU = 0.361890\n",
      "------\n",
      "INPUT:\n",
      "<sos> 加快 国际 通道 建设 . <eos>\n",
      "REF:\n",
      "<sos> we should step up the building of international communications . <eos>\n",
      "PREDICTION:\n",
      "<sos> speed up construction construction international construction . <eos>\n",
      "BLEU = 0.367828\n",
      "------\n",
      "INPUT:\n",
      "<sos> 这个 任务 没有 完成 . <eos>\n",
      "REF:\n",
      "<sos> this task has not been fulfilled . <eos>\n",
      "PREDICTION:\n",
      "<sos> this is not task . <eos>\n",
      "BLEU = 0.549413\n",
      "------\n",
      "INPUT:\n",
      "<sos> 北京 高度 关注 南丹 . <eos>\n",
      "REF:\n",
      "<sos> beijing has been highly concerned about nandan . <eos>\n",
      "PREDICTION:\n",
      "<sos> beijing is a high beijing beijing . <eos>\n",
      "BLEU = 0.516636\n",
      "------\n",
      "INPUT:\n",
      "<sos> 交往 方式 可以 多种多样 . <eos>\n",
      "REF:\n",
      "<sos> there can be a variety of ways of exchanges . <eos>\n",
      "PREDICTION:\n",
      "<sos> several exchanges can be used . <eos>\n",
      "BLEU = 0.412668\n",
      "------\n",
      "INPUT:\n",
      "<sos> \" 测试 正常 ! \" <eos>\n",
      "REF:\n",
      "<sos> \" all the tests are normal ! \" <eos>\n",
      "PREDICTION:\n",
      "<sos> \" testing normal normal normal ! \" <eos>\n",
      "BLEU = 0.317622\n",
      "------\n",
      "INPUT:\n",
      "<sos> \" 海洋 不是 <unk> . <eos>\n",
      "REF:\n",
      "<sos> \" the ocean is not a moat . <eos>\n",
      "PREDICTION:\n",
      "<sos> \" is not like anything . <eos>\n",
      "BLEU = 0.586405\n",
      "------\n",
      "INPUT:\n",
      "<sos> 图斯万迪 对此 表示 感谢 . <eos>\n",
      "REF:\n",
      "<sos> toeswandi expressed thanks for this . <eos>\n",
      "PREDICTION:\n",
      "<sos> the thank for this . <eos>\n",
      "BLEU = 0.376850\n",
      "------\n",
      "INPUT:\n",
      "<sos> 诡辩 改变 不了 事实 . <eos>\n",
      "REF:\n",
      "<sos> sophistry cannot change the facts . <eos>\n",
      "PREDICTION:\n",
      "<sos> facts facts cannot be carried in the . <eos>\n",
      "BLEU = 0.508133\n",
      "------\n",
      "INPUT:\n",
      "<sos> 这 决 不是 危言耸听 . <eos>\n",
      "REF:\n",
      "<sos> this is certainly not saying something frightening just to scare people . <eos>\n",
      "PREDICTION:\n",
      "<sos> this is just not just making this . <eos>\n",
      "BLEU = 0.277015\n",
      "------\n",
      "INPUT:\n",
      "<sos> 他 果断 地 回答 . <eos>\n",
      "REF:\n",
      "<sos> he answered firmly . <eos>\n",
      "PREDICTION:\n",
      "<sos> he was a answer for answer . <eos>\n",
      "BLEU = 0.577350\n",
      "------\n",
      "INPUT:\n",
      "<sos> 这是 毫无 道理 的 . <eos>\n",
      "REF:\n",
      "<sos> this is altogether without reason . <eos>\n",
      "PREDICTION:\n",
      "<sos> this is a truth . <eos>\n",
      "BLEU = 0.448153\n",
      "------\n",
      "INPUT:\n",
      "<sos> 王忠禹 主持 了 座谈会 . <eos>\n",
      "REF:\n",
      "<sos> wang zhongyu chaired the forum . <eos>\n",
      "PREDICTION:\n",
      "<sos> wang zhongyu presided over the forum . <eos>\n",
      "BLEU = 0.431670\n",
      "------\n",
      "INPUT:\n",
      "<sos> 美国 人 , 慌 ! <eos>\n",
      "REF:\n",
      "<sos> panic , americans ! <eos>\n",
      "PREDICTION:\n",
      "<sos> americans americans americans ! <eos>\n",
      "BLEU = 0.508133\n",
      "------\n",
      "INPUT:\n",
      "<sos> 首先 是 技术 支持 . <eos>\n",
      "REF:\n",
      "<sos> first of all , we will give technological support . <eos>\n",
      "PREDICTION:\n",
      "<sos> first , support technology . <eos>\n",
      "BLEU = 0.357909\n",
      "------\n",
      "Average BLEU = 0.46931946840112215\n"
     ]
    }
   ],
   "source": [
    "from infer_eval import bleu\n",
    "\n",
    "test_range = 50\n",
    "ave_bleu = 0\n",
    "for test_idx in range(test_range):\n",
    "    input_batches, input_lengths,\\\n",
    "        target_batches, target_lengths = random_batch(src,tgt,test_pairs,1,test_idx)\n",
    "    _, pred = net.inference(input_batches[:,0].reshape(input_lengths[0].item(),1),input_lengths[0].reshape(1))\n",
    "    inp = ' '.join([test_src.idx2w[t] for t in input_batches[:,0].cpu().numpy()])\n",
    "    mt = ' '.join([test_tgt.idx2w[t] for t in pred if t!= PAD_idx])\n",
    "    ref = ' '.join([test_tgt.idx2w[t] for t in target_batches[:,0].cpu().numpy() if t != PAD_idx])\n",
    "    print('INPUT:\\n' + inp)\n",
    "    print('REF:\\n' + ref)\n",
    "    print('PREDICTION:\\n' + mt)\n",
    "    tmp_score = bleu([mt],[[ref]],4)\n",
    "    ave_bleu += tmp_score\n",
    "    print('BLEU = %f' % tmp_score)\n",
    "    print(\"------\")\n",
    "print('Average BLEU = '+str(ave_bleu/test_range))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Input Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT:\n",
      "<sos> 国家主席 : 江泽民 . <eos>\n",
      "PREDICTION:\n",
      "<sos> president jiang zemin . <eos>\n"
     ]
    }
   ],
   "source": [
    "inputs = '国家主席 : 江泽民 .'\n",
    "user_var, user_len = user_input(inputs, src)\n",
    "_, pred = net.inference(user_var[:,0].reshape(user_len[0].item(),1),user_len[0].reshape(1))\n",
    "inp = ' '.join([src.idx2w[t] for t in user_var[:,0].cpu().numpy()])\n",
    "mt = ' '.join([tgt.idx2w[t] for t in pred if t!= PAD_idx])\n",
    "print('INPUT:\\n' + inp)\n",
    "print('PREDICTION:\\n' + mt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
