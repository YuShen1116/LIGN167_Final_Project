{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils import clip_grad_norm_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "\n",
    "# import basic lib\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import string\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "from io import open\n",
    "\n",
    "# import pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "# import loss func\n",
    "import masked_cross_entropy\n",
    "import copy\n",
    "# check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_precision(candidate, references, n):\n",
    "    count = 0\n",
    "    match_num = 0\n",
    "    len_c = len(candidate)\n",
    "    \n",
    "    for i in range(len_c):\n",
    "        ref_temp = []\n",
    "        for reference in references:\n",
    "            for k in range(len(reference)):\n",
    "                ref_sentence = reference[k]\n",
    "                words_ref_sentence = ref_sentence.strip()\n",
    "                words_ref_sentence = words_ref_sentence.split()\n",
    "                num_max = len(words_ref_sentence) - n + 1\n",
    "                ngram_temp = {}\n",
    "                for j in range(num_max):\n",
    "                    ngram = ' '.join(words_ref_sentence[j:j+n])\n",
    "                    ngram = ngram.lower()\n",
    "                    if ngram in ngram_temp.keys():\n",
    "                        ngram_temp[ngram] += 1\n",
    "                    else:\n",
    "                        ngram_temp[ngram] = 1\n",
    "                ref_temp.append(ngram_temp)\n",
    "            \n",
    "        cand_sentence = candidate[i]\n",
    "        words_cand = cand_sentence.strip()\n",
    "        words_cand = words_cand.split()\n",
    "        num_max_cand = len(words_cand) - n + 1\n",
    "        cand_temp = {}\n",
    "        for j in range(num_max_cand):\n",
    "            ngram = ' '.join(words_cand[j:j+n])\n",
    "            ngram = ngram.lower()\n",
    "            if ngram in cand_temp.keys():\n",
    "                cand_temp[ngram] += 1\n",
    "            else:\n",
    "                cand_temp[ngram] = 1\n",
    "        count += num_max_cand\n",
    "        match_num += match_counts(ref_temp, cand_temp)\n",
    "    if match_num != 0:\n",
    "        p = 1. * match_num / count\n",
    "    else:\n",
    "        p = 0\n",
    "    return p\n",
    "\n",
    "def match_counts(ref_counts, cand_temp):\n",
    "    num = 0\n",
    "    for ngram in cand_temp.keys():\n",
    "        count = cand_temp[ngram]\n",
    "        max_ref = 0\n",
    "        for ref in ref_counts:\n",
    "            if ngram in ref:\n",
    "                max_ref = max(max_ref, ref[ngram])\n",
    "        count = min(max_ref, count)\n",
    "        num = num + count\n",
    "    return num\n",
    "\n",
    "def brevity_penalty(candidate, references):\n",
    "    len_c = len(candidate)\n",
    "    r = 0\n",
    "    c = 0\n",
    "    for i in range(len_c):\n",
    "        ref_lens = []\n",
    "        for reference in references:\n",
    "            for k in range(len(reference)):\n",
    "                ref_sentence = reference[k]\n",
    "                words_ref_sentence = ref_sentence.strip()\n",
    "                words_ref_sentence = words_ref_sentence.split()\n",
    "                ref_lens.append(len(words_ref_sentence))\n",
    "        cand_sentence = candidate[i]\n",
    "        words_cand = cand_sentence.strip().split()\n",
    "        init_len_diff = abs(len(words_cand)-ref_lens[0])\n",
    "        best = ref_lens[0]\n",
    "        for num in ref_lens:\n",
    "            if (abs(len(words_cand)-num)) < init_len_diff:\n",
    "                init_len_diff = abs(len(words_cand) - num)\n",
    "                best = num\n",
    "        r = r + best\n",
    "        c = c + len(words_cand)\n",
    "    if c > r:\n",
    "        bp = 1\n",
    "    else:\n",
    "        bp = math.exp(1 - 1. * r / c)\n",
    "    return bp\n",
    "\n",
    "def bleu(candidate, references, n):\n",
    "    # n is the maximum length of each ngram you set \n",
    "    weight = 1./n\n",
    "    temp = 0\n",
    "    for i in range(n):\n",
    "        p = modified_precision(candidate, references, i + 1)\n",
    "        temp += math.log(p) * weight if p != 0 else 0\n",
    "    temp = math.exp(temp)\n",
    "    return brevity_penalty(candidate, references) * temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_idx = 0\n",
    "EOS_idx = 1\n",
    "UNK_idx = 2\n",
    "PAD_idx = 3\n",
    "\n",
    "USE_CUDA = True\n",
    "\n",
    "class Preprocessor:\n",
    "    '''\n",
    "    class for preprocessing\n",
    "    '''\n",
    "    def __init__(self, name):\n",
    "        '''\n",
    "        initialize vocab and counter\n",
    "        '''\n",
    "        self.name = name\n",
    "        self.w2idx = {\"<sos>\" : 0, \"<eos>\" : 1, \"<unk>\" : 2, \"<pad>\" : 3}\n",
    "        self.counter = {}\n",
    "        self.idx2w = {0: \"<sos>\", 1: \"<eos>\", 2:\"<unk>\", 3:\"<pad>\"}\n",
    "        self.num = 4\n",
    "\n",
    "    def SentenceAdder(self, sentence):\n",
    "        '''\n",
    "        Add a sentence to dataset\n",
    "        '''\n",
    "        for word in sentence.split(' '):\n",
    "            self.WordAdder(word)\n",
    "\n",
    "    def WordAdder(self, word):\n",
    "        '''\n",
    "        Add single word to dataset and update vocab and counter\n",
    "        '''\n",
    "        if word in self.w2idx:\n",
    "            self.counter[word] += 1\n",
    "        else:\n",
    "            self.w2idx[word] = self.num\n",
    "            self.counter[word] = 1\n",
    "            self.idx2w[self.num] = word\n",
    "            self.num += 1\n",
    "            \n",
    "    def trim(self, min_count=1):\n",
    "        '''\n",
    "        Trim to remove non-frequent word\n",
    "        '''\n",
    "        keep = []\n",
    "        for k, v in self.counter.items():\n",
    "            if v >= min_count: keep.append(k)\n",
    "        print(self.name+':')\n",
    "        print('Total words', len(self.w2idx))\n",
    "        print('After Trimming', len(keep))\n",
    "        print('Keep Ratio %', 100 * len(keep) / len(self.w2idx))\n",
    "        self.w2idx = {\"<sos>\" : 0, \"<eos>\" : 1, \"<unk>\" : 2, \"<pad>\" : 3}\n",
    "        self.counter = {}\n",
    "        self.idx2w = {0: \"<sos>\", 1: \"<eos>\", 2:\"<unk>\", 3:\"<pad>\"}\n",
    "        self.num = 4\n",
    "        for w in keep:\n",
    "            self.WordAdder(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Uni2Ascii(s):\n",
    "    '''\n",
    "    transfer from unicode to ascii\n",
    "    '''\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                    if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def StrCleaner(s):\n",
    "    '''\n",
    "    trim, delete non-letter and lowercase string\n",
    "    '''\n",
    "    s = Uni2Ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "def DataReader(path, lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open(path, encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    #pairs = [[StrCleaner(s) for s in l.split('<------>')] for l in lines]\n",
    "    pairs = [[s.lower() for s in l.split('<------>')] for l in lines]\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Preprocessor(lang2)\n",
    "        output_lang = Preprocessor(lang1)\n",
    "    else:\n",
    "        input_lang = Preprocessor(lang1)\n",
    "        output_lang = Preprocessor(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_LENGTH = 3\n",
    "MAX_LENGTH = 50\n",
    "\n",
    "def filterPair(p):\n",
    "    '''\n",
    "    Filter to get expected pairs with specific length\n",
    "    '''\n",
    "    return MIN_LENGTH <= len(p[0].split(' ')) <= MAX_LENGTH and \\\n",
    "        MIN_LENGTH <= len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(path, lang1, lang2, reverse=True):\n",
    "    input_lang, output_lang, pairs = DataReader(path, lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.SentenceAdder(pair[0])\n",
    "        output_lang.SentenceAdder(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.num)\n",
    "    print(output_lang.name, output_lang.num)\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "\n",
    "src, tgt, pairs = prepareData('data/train.txt', 'english', 'chinese')\n",
    "src.trim()\n",
    "tgt.trim()\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2idx(preprocessor, sentence):\n",
    "    '''\n",
    "    Read sentence and translate into word index plus eos\n",
    "    '''\n",
    "    return [SOS_idx] + [preprocessor.w2idx[w] if w in preprocessor.w2idx \\\n",
    "            else UNK_idx for w in sentence.split(' ')] + [EOS_idx]\n",
    "\n",
    "def pad(seq, max_len):\n",
    "    '''\n",
    "    Add padding to sentence with different length\n",
    "    '''\n",
    "    seq += [PAD_idx for i in range(max_len - len(seq))]\n",
    "    return seq\n",
    "\n",
    "def random_batch(src, tgt, pairs, batch_size, batch_idx):\n",
    "    '''\n",
    "    Randomly generate batch data\n",
    "    '''\n",
    "    inputs, target = [], []\n",
    "    \n",
    "    # Choose batch\n",
    "    for s in pairs[batch_idx*batch_size:(batch_idx+1)*batch_size]:\n",
    "        inputs.append(sentence2idx(src, s[0]))\n",
    "        target.append(sentence2idx(tgt, s[1]))\n",
    "        \n",
    "    # Sort by length\n",
    "    seq_pairs = sorted(zip(inputs, target), key=lambda p: len(p[0]), reverse=True)\n",
    "    inputs, target = zip(*seq_pairs)\n",
    "    \n",
    "    # Obtain length of each sentence and pad\n",
    "    input_lens = [len(s) for s in inputs]\n",
    "    input_max = max(input_lens)\n",
    "    input_padded = [pad(s, input_max) for s in inputs]\n",
    "    target_lens = [len(s) for s in target]\n",
    "    target_max = max(target_lens)\n",
    "    target_padded = [pad(s, target_max) for s in target]\n",
    "\n",
    "    # Create Variable\n",
    "    if USE_CUDA:\n",
    "        input_vars = Variable(torch.LongTensor(input_padded).cuda()).transpose(0, 1)\n",
    "        input_lens = Variable(torch.LongTensor(input_lens).cuda())\n",
    "        target_vars = Variable(torch.LongTensor(target_padded).cuda()).transpose(0, 1)\n",
    "        target_lens = Variable(torch.LongTensor(target_lens).cuda())\n",
    "    else:\n",
    "        input_vars = Variable(torch.LongTensor(input_padded)).transpose(0, 1)\n",
    "        input_lens = Variable(torch.LongTensor(input_lens))\n",
    "        target_vars = Variable(torch.LongTensor(target_padded)).transpose(0, 1)\n",
    "        target_lens = Variable(torch.LongTensor(target_lens))\n",
    "\n",
    "    return input_vars, input_lens, target_vars, target_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    '''\n",
    "    Define encoder and forward process\n",
    "    '''\n",
    "    def __init__(self, dim_input, dim_embed, dim_hidden, num_layers, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dim_input = dim_input\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.dim_embed = dim_embed\n",
    "        self.embed = nn.Embedding(dim_input, dim_embed)\n",
    "        self.cell = nn.GRU(dim_embed, dim_hidden, \n",
    "                          num_layers, dropout=dropout, \n",
    "                          bidirectional=True)\n",
    "        \n",
    "    def forward(self, inputs, inputs_lens, hidden=None):\n",
    "        '''\n",
    "        We need to sum the outputs since bi-diretional is used\n",
    "        '''\n",
    "        #print('e')\n",
    "        embedded = self.embed(inputs)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, inputs_lens)\n",
    "        outputs, hidden = self.cell(packed, hidden)\n",
    "        outputs, output_lengths = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        outputs = outputs[:, :, :self.dim_hidden] + \\\n",
    "                    outputs[:, :, self.dim_hidden:]\n",
    "        return outputs, hidden\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    '''\n",
    "    Define attention mechanism\n",
    "    '''\n",
    "    def __init__(self, dim_hidden):\n",
    "        super(Attention, self).__init__()\n",
    "        self.dim_hidden = dim_hidden\n",
    "        # 2*dim_hidden is needed since bi-direction is used\n",
    "        self.attn = nn.Linear(2*self.dim_hidden, dim_hidden)\n",
    "        self.v = nn.Parameter(torch.rand(dim_hidden))\n",
    "        stdv = 1. / math.sqrt(self.v.size(0))\n",
    "        self.v.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        #print('a')\n",
    "        timestep = encoder_outputs.size(0)\n",
    "        h = hidden.repeat(timestep, 1, 1).transpose(0, 1)\n",
    "        encoder_outputs = encoder_outputs.transpose(0, 1)\n",
    "        scores = self.score(h, encoder_outputs)\n",
    "        return F.relu(scores).unsqueeze(1)\n",
    "\n",
    "    def score(self, hidden, encoder_outputs):\n",
    "        e = F.softmax(self.attn(torch.cat([hidden, encoder_outputs], 2)),dim=1)\n",
    "        e = e.transpose(1, 2)\n",
    "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)\n",
    "        e = torch.bmm(v, e)\n",
    "        return e.squeeze(1)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    '''\n",
    "    Define decoder with attention\n",
    "    '''\n",
    "    def __init__(self, dim_embed, dim_hidden, dim_output, num_layers, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dim_embed = dim_embed\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.dim_output = dim_output\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embed = nn.Embedding(dim_output, dim_embed)\n",
    "        self.dropout = nn.Dropout(dropout, inplace=True)\n",
    "        self.attention = Attention(dim_hidden)\n",
    "        self.cell = nn.GRU(dim_hidden + dim_embed, dim_hidden,\n",
    "                          num_layers, dropout=dropout)\n",
    "        self.out = nn.Linear(2*dim_hidden, dim_output)\n",
    "\n",
    "    def forward(self, inputs, last_hidden, encoder_outputs):\n",
    "        \n",
    "#         print('input size ' + str(inputs.size()))\n",
    "#         print('input type ' + str(inputs.type()))\n",
    "        \n",
    "        embedded = self.embed(inputs).unsqueeze(0)  # (1,B,N)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "#         print('emmb size ' + str(embedded.size()))\n",
    "#         print('emmb type ' + str(embedded.type()))\n",
    "        \n",
    "#         input size torch.Size([1])\n",
    "# input type torch.cuda.LongTensor\n",
    "# emmb size torch.Size([1, 1, 256])\n",
    "# emmb type torch.cuda.FloatTensor\n",
    "        # Calculate attention weights and apply to encoder outputs\n",
    "        attn_weights = self.attention(last_hidden[-1], encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))  # (B,1,N)\n",
    "        context = context.transpose(0, 1)  # (1,B,N)\n",
    "        # Combine embedded input word and attended context, run through RNN\n",
    "\n",
    "        rnn_input = torch.cat([embedded, context], 2)\n",
    "        output, hidden = self.cell(rnn_input, last_hidden)\n",
    "        output = output.squeeze(0)  # (1,B,N) -> (B,N)\n",
    "        context = context.squeeze(0)\n",
    "        output = self.out(torch.cat([output, context], 1))\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, src_len, tgt, tgt_len, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.size(1)\n",
    "        max_len = tgt.size(0)\n",
    "        vocab_size = self.decoder.dim_output\n",
    "        if USE_CUDA:\n",
    "            outputs = Variable(torch.zeros(max_len, batch_size, vocab_size).cuda())\n",
    "        else:\n",
    "            outputs = Variable(torch.zeros(max_len, batch_size, vocab_size))\n",
    "        encoder_output, hidden = self.encoder(src, src_len)\n",
    "        hidden = hidden[:self.decoder.num_layers]\n",
    "        # Put <sos> at first position\n",
    "        if USE_CUDA:\n",
    "            output = Variable(tgt.data[0, :].cuda())\n",
    "        else:\n",
    "            output = Variable(tgt.data[0, :])\n",
    "        for t in range(1, max_len):\n",
    "            output, hidden, attn_weights = self.decoder(\n",
    "                    output, hidden, encoder_output)\n",
    "            outputs[t] = output\n",
    "            # Randomly choose whether to use teacher force or not\n",
    "            is_teacher = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.data.max(1)[1]\n",
    "            if USE_CUDA:\n",
    "                output = Variable(tgt.data[t].cuda() if is_teacher else top1.cuda())\n",
    "            else:\n",
    "                output = Variable(tgt.data[t] if is_teacher else top1)\n",
    "        return outputs\n",
    "    \n",
    "    def inference(self, src, src_len, max_len = MAX_LENGTH):\n",
    "        pred_idx = []\n",
    "        batch_size = src.size(1)\n",
    "        vocab_size = self.decoder.dim_output\n",
    "        if USE_CUDA:\n",
    "            outputs = Variable(torch.zeros(max_len, batch_size, vocab_size).cuda())\n",
    "        else:\n",
    "            outputs = Variable(torch.zeros(max_len, batch_size, vocab_size))\n",
    "        \n",
    "        encoder_output, hidden = self.encoder(src, src_len)\n",
    "        hidden = hidden[:self.decoder.num_layers]\n",
    "        # Put <sos> at first position\n",
    "        if USE_CUDA:\n",
    "            output = Variable(src.data[0, :].cuda())\n",
    "        else:\n",
    "            output = Variable(src.data[0, :])\n",
    "        pred_idx.append(SOS_idx)\n",
    "        for t in range(1, max_len):\n",
    "            output, hidden, attn_weights = self.decoder(\n",
    "                    output, hidden, encoder_output)\n",
    "            outputs[t] = output\n",
    "            top1 = output.data.max(1)[1]\n",
    "            pred_idx.append(top1.item())\n",
    "            if USE_CUDA:\n",
    "                output = Variable(top1.cuda())\n",
    "            else:\n",
    "                output = Variable(top1)\n",
    "            if top1 == EOS_idx: break\n",
    "        return outputs, pred_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "hidden_size = 512\n",
    "embed_size = 256\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 1 \n",
    "# n_layers = 4\n",
    "width = 5\n",
    "encoder_test = Encoder(src.num, embed_size, hidden_size, encoder_n_layers, dropout=0.2)\n",
    "decoder_test = Decoder(embed_size, hidden_size, tgt.num, decoder_n_layers, dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Seq2Seq(encoder_test,decoder_test).cuda()\n",
    "# opt = optim.Adam(net.parameters(),lr=0.0001)\n",
    "net.load_state_dict(torch.load('./saved_model_1.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearch(nn.Module):\n",
    "    '''\n",
    "    Implement BeamSearch for testing\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, encoder, decoder, width):\n",
    "        super(BeamSearch, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.width = width\n",
    "        \n",
    "    def forward(self, src, src_len, width, max_len):\n",
    "        vocab_size = self.decoder.dim_output        \n",
    "        encoder_output, hidden = self.encoder(src, src_len)\n",
    "        hidden = hidden[:self.decoder.num_layers]\n",
    "        \n",
    "        if USE_CUDA:\n",
    "            #output is in the shape of batch_size * dict size\n",
    "            beam_best = Variable(torch.zeros(width, max_len)).cuda()\n",
    "            prob_best = Variable(torch.zeros(width)).cuda()\n",
    "            #beam options, all possible width * width options\n",
    "            beam_options = Variable(torch.zeros(width * width, max_len)).cuda()\n",
    "            prob_options = Variable(torch.zeros(width * width)).cuda()\n",
    "            output = Variable(src.data[0, :].cuda())\n",
    "            temp = Variable(src.data[0, :].cuda())\n",
    "        else:\n",
    "            beam_best = Variable(torch.zeros(width, max_len))\n",
    "            prob_best = Variable(torch.zeros(width))\n",
    "            beam_options = Variable(torch.zeros(width * width, max_len))\n",
    "            prob_options = Variable(torch.zeros(width * width))\n",
    "            temp = Variable(src.data[0, :])\n",
    "            output = Variable(src.data[0, :])\n",
    "        \n",
    "        output, _, _ = self.decoder(\n",
    "            output, hidden, encoder_output)\n",
    "        \n",
    "        #first round\n",
    "        #val: prob, idx: index\n",
    "        val, idx = output.data.topk(k = width, dim = 1)\n",
    "  \n",
    "        #generate beams\n",
    "        for i in range(width):\n",
    "            beam_best[i][1] = idx[0][i]\n",
    "            prob_best[i] = val[0][i].exp() #since prob log,softmax from -10 to -12\n",
    "        \n",
    "        for t in range(2, max_len):\n",
    "            cnt = 0\n",
    "            for i in range(width):\n",
    "                curr_prob = prob_best[i]\n",
    "                #\n",
    "                hidden = self.encoder(src, src_len)[1][:self.decoder.num_layers] \n",
    "                \n",
    "                #feed the entire vector for the training process\n",
    "                for j in range(t):\n",
    "                    temp = beam_best[i][j].reshape(1).long()\n",
    "                    new_output, hidden, _ = self.decoder(\n",
    "                            temp, hidden, encoder_output)\n",
    "                \n",
    "                val1, idx1 = new_output.data.topk(k = width, dim = 1)\n",
    "                \n",
    "                for j in range(width):\n",
    "                    beam_options[cnt] = beam_best[i]\n",
    "                    beam_options[cnt][t] = idx1[0][j]\n",
    "                    prob_options[cnt] = val1[0][j].exp() * curr_prob\n",
    "                    cnt += 1\n",
    "            \n",
    "            topVal, topInx = prob_options.topk(k = width, dim = 0)\n",
    "            for j in range(topInx.size(0)):\n",
    "                prob_best[j] = topVal[j]\n",
    "                beam_best[j] = beam_options[topInx[j]]\n",
    "            \n",
    "            \n",
    "        best_index = prob_best.max(0)[1]\n",
    "        return beam_options[best_index].cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_widths = [1,2,3,4]\n",
    "net = BeamSearch(net.encoder,net.decoder, width).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate bleu score on training set\n",
    "\n",
    "net.eval()\n",
    "pairs.sort(key=lambda x: len(x[0].split()))\n",
    "\n",
    "#77083 pairs\n",
    "for i in beam_widths:\n",
    "    blue_score = []\n",
    "    for index_sample in range(len(pairs)):\n",
    "        input_batches, input_lengths,\\\n",
    "            target_batches, target_lengths = random_batch(src,tgt,pairs,1,index_sample)\n",
    "\n",
    "        for test_idx in range(1):\n",
    "            pred = net(input_batches[:,test_idx].reshape(input_lengths[0].item(),1), input_lengths[0].reshape(1), i, MAX_LENGTH)\n",
    "            inp = ' '.join([src.idx2w[t] for t in input_batches[:,test_idx].cpu().numpy()])\n",
    "            mt = ' '.join([tgt.idx2w[t] for t in pred if t!= PAD_idx])\n",
    "            idx = mt.find('<eos>')\n",
    "            mt = mt[:idx+5]\n",
    "            ref = ' '.join([tgt.idx2w[t] for t in target_batches[:,test_idx].cpu().numpy() if t != PAD_idx])\n",
    "            blue_score.append(bleu([mt],[[ref]],4))\n",
    "            \n",
    "        print('INPUT:\\n' + inp)\n",
    "        print('REF:\\n' + ref)\n",
    "        print('PREDICTION:\\n' + mt)\n",
    "        print(\"------\")\n",
    "    print(str(i) + \" finished: \" + str(np.mean(blue_score)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testSet\n",
    "\n",
    "test_src, test_tgt, test_pairs = prepareData('data/test.txt', 'english', 'chinese')\n",
    "test_src.trim()\n",
    "test_tgt.trim()\n",
    "print(random.choice(test_pairs))\n",
    "\n",
    "test_src.w2idx, test_src.idx2w, test_src.num = src.w2idx, src.idx2w, src.num\n",
    "test_tgt.w2idx, test_tgt.idx2w, test_tgt.num = tgt.w2idx, tgt.idx2w, tgt.num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bleu score on testSet\n",
    "\n",
    "net.eval()\n",
    "test_pairs.sort(key=lambda x: len(x[0].split()))\n",
    "\n",
    "#8572 pairs\n",
    "for i in beam_widths:\n",
    "    blue_score = []\n",
    "    for index_sample in range(len(test_pairs)):\n",
    "        input_batches, input_lengths,\\\n",
    "            target_batches, target_lengths = random_batch(test_src,test_tgt,test_pairs,1,index_sample)\n",
    "\n",
    "        for test_idx in range(1):\n",
    "            pred = net(input_batches[:,test_idx].reshape(input_lengths[0].item(),1), input_lengths[0].reshape(1), i, MAX_LENGTH)\n",
    "            inp = ' '.join([test_src.idx2w[t] for t in input_batches[:,test_idx].cpu().numpy()])\n",
    "            mt = ' '.join([test_tgt.idx2w[t] for t in pred if t!= PAD_idx])\n",
    "            idx = mt.find('<eos>')\n",
    "            mt = mt[:idx+5]\n",
    "            ref = ' '.join([test_tgt.idx2w[t] for t in target_batches[:,test_idx].cpu().numpy() if t != PAD_idx])\n",
    "            blue_score.append(bleu([mt],[[ref]],4))\n",
    "            \n",
    "        print('INPUT:\\n' + inp)\n",
    "        print('REF:\\n' + ref)\n",
    "        print('PREDICTION:\\n' + mt)\n",
    "        print(\"------\")\n",
    "    print(str(i) + \" finished: \" + str(np.mean(blue_score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
